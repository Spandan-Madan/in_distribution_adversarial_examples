{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vertical-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function, division\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import antialiased_cnns\n",
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "sys.path.append('/om5/user/smadan/training_scaffold_own/res/')\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import wandb\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--num_epochs', type = int, default = 50)\n",
    "# parser.add_argument('--model_arch', type = str, default = 'resnet18')\n",
    "# parser.add_argument('--batch_size', type = int, default = 100)\n",
    "# parser.add_argument('--num_classes', type = int, default = 11)\n",
    "# parser.add_argument('--base_lr', type = float, default = 0.001)\n",
    "# parser.add_argument('--use_gpu', type = bool, default = True)\n",
    "# parser.add_argument('--run_name', type = str)\n",
    "# parser.add_argument('--pretrained', action='store_true')\n",
    "# parser.add_argument('--image_size', type = int, default = 224)\n",
    "# parser.add_argument('--dataset_name', type = str, required = True)\n",
    "# parser.add_argument('--normalize', action='store_true')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# wandb_config = dict(vars(args))\n",
    "wandb_config = {'num_epochs': 50, 'model_arch': 'resnet18', 'batch_size': 125, 'num_classes': 11, 'base_lr': 0.0003, 'use_gpu': True, 'run_name': 'resnet18_v7_normalized', 'pretrained': False, 'image_size': 224, 'dataset_name': 'image_train_v7_shapenet', 'normalize': True} \n",
    "\n",
    "# wandb.login()\n",
    "# NUM_EPOCHS = args.num_epochs\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# MODEL_ARCH = args.model_arch\n",
    "# NUM_CLASSES = args.num_classes\n",
    "# BASE_LR = args.base_lr\n",
    "# USE_GPU = args.use_gpu\n",
    "# SAVE_PATH = args.save_path\n",
    "# LOG_FILE = args.log_file\n",
    "# DATASET_NAME = args.dataset_name\n",
    "\n",
    "\n",
    "# LOG_FILE_HANDLE = open(LOG_FILE, 'w')\n",
    "\n",
    "loader = get_loader('multi_attribute_loader_file_list_shapenet')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ideal-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "input_img_size = wandb_config['image_size']\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#### Below was implemented for LeNet, leave commented ####\n",
    "# image_transform = transforms.Compose([\n",
    "#     transforms.Resize((input_img_size,input_img_size)),\n",
    "#     transforms.Grayscale(),\n",
    "#     transforms.ToTensor()\n",
    "# #     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "loader_new = get_loader('multi_attribute_loader_file_list_shapenet')\n",
    "file_list_root = '/om5/user/smadan/dataset_lists_openmind'\n",
    "att_path = '/om5/user/smadan/differentiable_graphics_ml/training_models/shapenet_id_to_class_num.p'\n",
    "shuffles = {'train':True,'val':True,'test':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "direct-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}\n",
    "\n",
    "file_lists = {}\n",
    "dsets = {}\n",
    "dset_loaders = {}\n",
    "dset_sizes = {}\n",
    "for phase in ['train','test']:\n",
    "    file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,wandb_config['dataset_name'])\n",
    "    dsets[phase] = loader_new(file_lists[phase],att_path, image_transform)\n",
    "    dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=wandb_config['batch_size'], shuffle = shuffles[phase], num_workers=2,drop_last=True)\n",
    "    dset_sizes[phase] = len(dsets[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-aviation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

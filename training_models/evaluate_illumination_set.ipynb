{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Purpose\n",
    "\n",
    "Evaluating the performance of the network when lighting is moved around in a sphere. Data is not rendered here, simply forward propagated through the network and stored for later visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/storage001.ib.cluster/om2/user/smadan/differentiable_graphics_ml/training_models\n",
      "/om5/user/smadan/training_scaffold_own/res/loader/multi_attribute_loader.py\n",
      "/om5/user/smadan/training_scaffold_own/res/loader\n",
      "/om5/user/smadan/training_scaffold_own/res/loader/loader.py\n",
      "/om5/user/smadan/training_scaffold_own/res/loader\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function, division\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "sys.path.append('/om5/user/smadan/training_scaffold_own/res/')\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--num_epochs', type = int, default = 50)\n",
    "# parser.add_argument('--model_arch', type = str, default = 'simple_cnn')\n",
    "# parser.add_argument('--batch_size', type = int, default = 100)\n",
    "# parser.add_argument('--num_classes', type = int, default = 11)\n",
    "# parser.add_argument('--base_lr', type = float, default = 0.001)\n",
    "# parser.add_argument('--use_gpu', type = bool, default = True)\n",
    "# parser.add_argument('--run_name', type = str)\n",
    "# parser.add_argument('--pretrained', type = bool, default = False)\n",
    "\n",
    "# parser.add_argument('--dataset_name', type = str, required = True)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# wandb_config = dict(vars(args))\n",
    "# wandb.login()\n",
    "# NUM_EPOCHS = args.num_epochs\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# MODEL_ARCH = args.model_arch\n",
    "# NUM_CLASSES = args.num_classes\n",
    "# BASE_LR = args.base_lr\n",
    "# USE_GPU = args.use_gpu\n",
    "# SAVE_PATH = args.save_path\n",
    "# LOG_FILE = args.log_file\n",
    "# DATASET_NAME = args.dataset_name\n",
    "\n",
    "\n",
    "# LOG_FILE_HANDLE = open(LOG_FILE, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config = {}\n",
    "wandb_config['dataset_name'] = 'shapenet_illumination_large'\n",
    "wandb_config['batch_size'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_new = get_loader('multi_attribute_loader_file_list_shapenet')\n",
    "file_list_root = '/om5/user/smadan/dataset_lists_openmind'\n",
    "att_path = '/om5/user/smadan/differentiable_graphics_ml/training_models/shapenet_id_to_class_num.p'\n",
    "shuffles = {'test':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if wandb_config['use_gpu']:\n",
    "#     torch.cuda.set_device(0)\n",
    "\n",
    "count=0\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}\n",
    "\n",
    "file_lists = {}\n",
    "dsets = {}\n",
    "dset_loaders = {}\n",
    "dset_sizes = {}\n",
    "for phase in ['test']:\n",
    "    file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,wandb_config['dataset_name'])\n",
    "    dsets[phase] = loader_new(file_lists[phase],att_path, image_transform)\n",
    "    dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=wandb_config['batch_size'], shuffle = shuffles[phase], num_workers=2,drop_last=True)\n",
    "    dset_sizes[phase] = len(dsets[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load('/om5/user/smadan/differentiable_graphics_ml/training_models/saved_models/alexnet_10_models_00001_3_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648/648 [00:36<00:00, 17.76it/s]\n"
     ]
    }
   ],
   "source": [
    "light_point_to_matches = {}\n",
    "\n",
    "for data in tqdm(dset_loaders['test']):\n",
    "    inputs, labels, image_paths = data\n",
    "    inputs = inputs.float().cuda()\n",
    "    labels = labels.long().cuda()\n",
    "\n",
    "    outputs = loaded_model(inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    for i in range(wandb_config['batch_size']):\n",
    "        image_name = image_paths[i].split('/')[-1]\n",
    "        category, instance, light_x, light_y, light_z = image_name.split('_')\n",
    "        light_z = light_z.split('.png')[0]\n",
    "        light_point = \"%s_%s_%s\"%(light_x, light_y, light_z)\n",
    "        match = labels[i] == preds[i]\n",
    "        if light_point in light_point_to_matches.keys():\n",
    "            light_point_to_matches[light_point].append(match.item())\n",
    "        else:\n",
    "            light_point_to_matches[light_point] = [match.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_point_to_accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for light_point in light_point_to_matches.keys():\n",
    "    matches = light_point_to_matches[light_point]\n",
    "    accuracy = torch.sum(torch.tensor(matches))/float(len(matches))\n",
    "    light_point_to_accuracies[light_point] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('light_point_to_accuracies_large.p','wb') as F:\n",
    "    pickle.dump(light_point_to_accuracies,F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "machine_path = os.getcwd()\n",
    "user_root_dir = '/'.join(machine_path.split('/')[:-2])\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "sys.path.append('%s/training_scaffold_own/res/'%user_root_dir)\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Learning rate parameters\n",
    "BASE_LR = 0.001\n",
    "EPOCH_DECAY = 30 # number of epochs after which the Learning rate is decayed exponentially.\n",
    "DECAY_WEIGHT = 0.1 # factor by which the learning rate is reduced.\n",
    "\n",
    "# DATASET INFO\n",
    "NUM_CLASSES = 2 # set the number of classes in your dataset\n",
    "DATA_DIR = '../data/' # to run with the sample dataset, just set to 'hymenoptera_data'\n",
    "\n",
    "# DATALOADER PROPERTIES\n",
    "BATCH_SIZE = 10 # Set as high as possible. If you keep it too high, you'll get an out of memory error.\n",
    "\n",
    "\n",
    "### GPU SETTINGS\n",
    "CUDA_DEVICE = 0 # Enter device ID of your gpu if you want to run on gpu. Otherwise neglect.\n",
    "GPU_MODE = 1 # set to 1 if want to run on gpu.\n",
    "\n",
    "# MODEL_ARCH = 'transformer'\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'image_test_v7_cma_resnet18_antialiased_v7_40_normalized_final'\n",
    "model_path = '../training_models/saved_models/resnet18_antialiased_v7_40_normalized_final.pt'\n",
    "\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_normalized.pt'\n",
    "\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7_40'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_40_final.pt'\n",
    "\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7_anti_aliased'\n",
    "# model_path = '../training_models/saved_models/resnet18_antialiased_v7_normalized_final.pt'\n",
    "\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7_truly_shift_invariant'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_truly_shift_invariant_normalized_final.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7_subsampled_unseen'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_subsampled_normalized_final.pt'\n",
    "\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7_unseen'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_normalized.pt'\n",
    "\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7_40_unseen'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_40_final.pt'\n",
    "\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7_anti_aliased_unseen'\n",
    "# model_path = '../training_models/saved_models/resnet18_antialiased_v7_normalized_final.pt'\n",
    "\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7_truly_shift_invariant_unseen'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_truly_shift_invariant_normalized_final.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "GPU = 1\n",
    "\n",
    "NUM_CLASSES = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_new = get_loader('multi_attribute_loader_file_list_shapenet')\n",
    "if 'smadan' in user_root_dir:\n",
    "    file_list_root = '%s/dataset_lists_openmind'%user_root_dir\n",
    "elif 'spandan' in user_root_dir:\n",
    "    file_list_root = \"%s/dataset_lists_fasrc\"%user_root_dir\n",
    "\n",
    "\n",
    "att_path = '%s/differentiable_graphics_ml/training_models/shapenet_id_to_class_num.p'%user_root_dir\n",
    "shuffles = {'train':True,'val':True,'test':True,'seen_test':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ GET FROM USER CONFIG - TODO #####################\n",
    "file_lists = {}\n",
    "dsets = {}\n",
    "dset_loaders = {}\n",
    "dset_sizes = {}\n",
    "for phase in ['test']:\n",
    "    file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,DATASET_NAME)\n",
    "    dsets[phase] = loader_new(file_lists[phase],att_path, image_transform)\n",
    "    dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=BATCH_SIZE, shuffle = shuffles[phase], num_workers=0,drop_last=True)\n",
    "    dset_sizes[phase] = len(dsets[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# os.listdir('../training_models/saved_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '../training_models/saved_models/resnet18_train_v3_fixed_lighting.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_normalized.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v3_symmetric_bias_6_conditions_camera.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_normalized.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_fine_tuned_less_frozen.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_subsampled_normalized_final.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_truly_shift_invariant_normalized_final.pt'\n",
    "\n",
    "\n",
    "# model_path = '../training_models/saved_models/resnet18_v6_normalized_final.pt'\n",
    "loaded_model = torch.load(model_path)\n",
    "loaded_model.eval();\n",
    "loaded_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_shift_batch(arr, w_shift, h_shift):\n",
    "    empty_arr = torch.zeros((arr.shape))\n",
    "    \n",
    "    if w_shift <0 and h_shift<0:\n",
    "        empty_arr[:,:,:224+h_shift,:224+w_shift] = arr[:,:,-h_shift:,-w_shift:]\n",
    "    elif w_shift >=0 and h_shift >=0:\n",
    "        empty_arr[:,:,h_shift:,w_shift:] = arr[:,:,:arr.shape[2]-h_shift,:arr.shape[3]-w_shift]\n",
    "    elif w_shift >=0 and h_shift <0:\n",
    "        empty_arr[:,:,:224+h_shift,w_shift:] = arr[:,:,-h_shift:,:arr.shape[3]-w_shift]\n",
    "    elif w_shift <0 and h_shift >=0:\n",
    "        empty_arr[:,:,h_shift:,:224+w_shift] = arr[:,:,:arr.shape[2]-h_shift,-w_shift:]\n",
    "    \n",
    "    return empty_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\n",
      "10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [00:01<00:03,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\n",
      "10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [00:03<00:01,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\n",
      "10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "all_errors = 0\n",
    "total = 0\n",
    "for phase in ['test']:\n",
    "    for data in tqdm(dset_loaders[phase]):\n",
    "        count = 0\n",
    "        batch_predictions = torch.zeros((16, BATCH_SIZE))\n",
    "        for w_shift in range(-2, 2):\n",
    "            for h_shift in range(-2, 2):\n",
    "                if count % 10 == 0:\n",
    "                    print(\"%s\"%count)\n",
    "                inputs_orig, labels, image_paths = data\n",
    "                inputs_shifted = input_shift_batch(inputs_orig, w_shift, h_shift)\n",
    "                inputs = inputs_shifted.float().cuda()\n",
    "                labels = labels.long().cuda()\n",
    "                \n",
    "                im_means = torch.mean(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "                im_stds = torch.std(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "                inputs = (inputs - im_means)/im_stds\n",
    "\n",
    "                outputs = loaded_model(inputs)\n",
    "                preds = torch.argmax(outputs,dim=1)\n",
    "                batch_predictions[count] = preds\n",
    "                count += 1\n",
    "                batch_repeated_labels = labels.repeat(16, 1)\n",
    "        shifted_matches = batch_predictions.cpu() == batch_repeated_labels.cpu()\n",
    "        batch_errors = torch.sum(torch.sum(shifted_matches,dim=0) != len(shifted_matches))\n",
    "        all_errors += batch_errors\n",
    "        total += data[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9200)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - all_errors/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_repeated_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6800)\n"
     ]
    }
   ],
   "source": [
    "all_matches_stacked= torch.hstack(all_matches)\n",
    "shift_attack_accuracy = torch.sum(torch.sum(all_matches_stacked, dim=0) == 100)/100\n",
    "print(shift_attack_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-22c5b68c6888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mall_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om5/user/smadan/miniconda3/envs/diff_rendering_ml/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "all_preds = torch.zeros((100, BATCH_SIZE))\n",
    "count = 0\n",
    "for w_shift in range(-5,5):\n",
    "    for h_shift in range(-5,5):\n",
    "        if count % 10 == 0:\n",
    "            print(\"%s/100\"%count)\n",
    "        inputs_orig, labels, image_paths = data\n",
    "        inputs_shifted = input_shift_batch(inputs_orig, w_shift, h_shift)\n",
    "        inputs = inputs_shifted.float().cuda()\n",
    "        labels = labels.long().cuda()\n",
    "        if 'normalize' in model_path:\n",
    "            im_means = torch.mean(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            im_stds = torch.std(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            inputs = (inputs - im_means)/im_stds\n",
    "            \n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.argmax(outputs,dim=1)\n",
    "        all_preds[count] = preds        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n",
      "0.88\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print(torch.sum(all_corrects[i,:]).item()/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_corrects = 0\n",
    "# total = 0\n",
    "# incorrect_predicted_images = []\n",
    "# all_paths = []\n",
    "# for phase in ['test']:\n",
    "#     for data in tqdm(dset_loaders[phase]):\n",
    "#         inputs, labels, image_paths = data\n",
    "#         inputs = inputs.float().cuda()\n",
    "#         labels = labels.long().cuda()\n",
    "#         if 'normalize' in model_path:\n",
    "#             im_means = torch.mean(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "#             im_stds = torch.std(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "#             inputs = (inputs - im_means)/im_stds\n",
    "\n",
    "#         outputs = loaded_model(inputs)\n",
    "#         preds = torch.argmax(outputs,dim=1)\n",
    "#         corrects = torch.sum(preds == labels).item()\n",
    "#         all_corrects += corrects\n",
    "#         total += len(preds)\n",
    "#         all_paths.extend(image_paths)\n",
    "#         incorrect_predicted_images.extend([image_paths[i] for i in torch.where(preds!=labels)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6301522842639594\n"
     ]
    }
   ],
   "source": [
    "print(all_corrects/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = all_corrects/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 3D models seen: 0.6301522842639594\n"
     ]
    }
   ],
   "source": [
    "print('10 3D models seen: %s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3D models seen: 0.5473096446700507\n"
     ]
    }
   ],
   "source": [
    "print('4 3D models seen: %s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = {}\n",
    "model_predictions['correct'] = [i for i in all_paths if i not in incorrect_predicted_images]\n",
    "model_predictions['incorrect'] = incorrect_predicted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4599"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_predictions['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5451"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_predictions['incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45761194029850744"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_predictions['correct'])/len(all_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/om5/user/smadan/differentiable_graphics_ml/data/train_v7_shapenet/resnet18_v7_fine_tuned_less_frozen_v7_test.p'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/om5/user/smadan/differentiable_graphics_ml/data/train_v7_shapenet/%s_v7_test.p'%model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/om5/user/smadan/differentiable_graphics_ml/data/train_v7_shapenet/%s_v7_test.p'%model_name, 'wb') as F:\n",
    "    pickle.dump(model_predictions, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/om5/user/smadan/differentiable_graphics_ml/data/train_v7_shapenet/resnet18_incorrect_predictions.p', 'wb') as F:\n",
    "#     pickle.dump(incorrect_predicted_images, F)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 512, 7, 7])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

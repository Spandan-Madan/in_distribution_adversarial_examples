{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fleet-shore",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import utils\n",
    "import pickle\n",
    "import utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import MLP\n",
    "from utils import CMA_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dried-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "flexible-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as D\n",
    "\n",
    "import numpy as np\n",
    "import cma\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brown-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_uniform(batch_size, N_dim, r1, r2):\n",
    "    random_matrix = (r1 - r2) * torch.rand([batch_size, N_dim]) + r2\n",
    "    return random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latin-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, data_dimensions,name,seed=None):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        else:\n",
    "            print(\"Torch seed:%s\"%torch.seed())\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(data_dimensions, int(data_dimensions)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(int(data_dimensions), 2)\n",
    "        )\n",
    "        self.name = name\n",
    "        self.train_accuracy = 0\n",
    "        self.test_accuracy = 0\n",
    "        self.dataset = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contrary-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMA_info():\n",
    "    def __init__(self, model_name, num_samples):\n",
    "        super(CMA_info, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.distances = []\n",
    "        self.in_dist_advs = []\n",
    "        self.advs = []\n",
    "        self.starts = []\n",
    "    \n",
    "    def summary(self):\n",
    "        print('****************** CMA Summary *******************')\n",
    "        print('Trained on %s points:'%self.model_name.split('_')[-1])\n",
    "        print('Adversarials: %s/%s'%(len(self.advs), self.num_samples))\n",
    "        print('In-distribution: %s/%s'%(len(self.in_dist_advs), len(self.advs)))\n",
    "        \n",
    "        avg_dist = np.mean(self.distances)\n",
    "        print('Average L2 distance: %s'%(avg_dist))\n",
    "        print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "young-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_min = -10\n",
    "x1_max = 10\n",
    "\n",
    "x2_min = 20\n",
    "x2_max = 40\n",
    "\n",
    "test_min = -1\n",
    "test_max= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clean-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataset_size):\n",
    "    sample_1 = sample_uniform(dataset_size, N_dim, x1_min, x1_max)\n",
    "    sample_2 = sample_uniform(dataset_size, N_dim, x2_min, x2_max)\n",
    "    \n",
    "    labels_1 = torch.zeros(len(sample_1))\n",
    "    labels_2 = torch.ones(len(sample_2))\n",
    "    \n",
    "    X = torch.vstack([sample_1, sample_2])\n",
    "    Y = torch.hstack([labels_1, labels_2])\n",
    "    ids = list(range(len(X)))\n",
    "\n",
    "    random.shuffle(ids)\n",
    "    train_ids = ids[:int(0.8*len(X))]\n",
    "    test_ids = ids[int(0.8*len(X)):]\n",
    "\n",
    "    # len(X)\n",
    "\n",
    "    X_train = X[train_ids]\n",
    "    X_test = X[test_ids]\n",
    "\n",
    "    Y_train = Y[train_ids]\n",
    "    Y_test = Y[test_ids]\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floppy-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_make_dataset(dataset_size, num_chunks):\n",
    "    chunk_dataset_size = int(dataset_size/num_chunks)\n",
    "    for i in range(num_chunks):\n",
    "        print('sampling chunk...')\n",
    "        chunk_dataset = make_dataset(chunk_dataset_size)\n",
    "        print('writing chunk...')\n",
    "        with open('chunked_datasets/chunk_dataset_%s.p'%i,'wb') as F:\n",
    "            pickle.dump(chunk_dataset, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fresh-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(N_dim, dataset, name, seed = None):\n",
    "    X_train, X_test, Y_train, Y_test = dataset\n",
    "    X_train = X_train.cuda()\n",
    "    X_test = X_test.cuda()\n",
    "    Y_train = Y_train.cuda()\n",
    "    Y_test = Y_test.cuda()\n",
    "    \n",
    "    if seed is not None:\n",
    "        model = MLP(N_dim, name, seed).cuda()\n",
    "    else:\n",
    "        model = MLP(N_dim, name).cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.dataset = dataset\n",
    "    \n",
    "    for epoch in tqdm(range(100)): \n",
    "        outputs = model(X_train)\n",
    "        loss = loss_fn(outputs, Y_train.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predictions = torch.argmax(model(X_test), dim=1)\n",
    "        accuracy = torch.sum(predictions == Y_test)/len(Y_test)\n",
    "        \n",
    "        train_predictions = torch.argmax(model(X_train), dim=1)\n",
    "        train_accuracy = torch.sum(train_predictions == Y_train)/len(Y_train)\n",
    "\n",
    "        if epoch == 999:\n",
    "            print(\"Epoch:%s, Train Acc:%s\"%(epoch, train_accuracy))\n",
    "            print(\"Epoch:%s, Test Acc:%s\"%(epoch, accuracy))\n",
    "        \n",
    "        model.test_accuracy = accuracy\n",
    "        model.train_accuracy = train_accuracy\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "flexible-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_train_model_from_disk(N_dim, dataset_chunk_folder, name, seed = None):   \n",
    "    if seed is not None:\n",
    "        model = MLP(N_dim, name, seed).cuda()\n",
    "    else:\n",
    "        model = MLP(N_dim, name).cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    BATCH_SIZE = 64000\n",
    "    chunked_dataset_names = os.listdir(dataset_chunk_folder)\n",
    "    num_chunked_datasets = len(chunked_dataset_names)\n",
    "    \n",
    "    print('Found %s chunks'%num_chunked_datasets)\n",
    "    \n",
    "    for epoch in tqdm(range(10)):\n",
    "        for i in range(num_chunked_datasets):\n",
    "            print('Loading chunk...')\n",
    "            with open('%s/%s'%(dataset_chunk_folder, chunked_dataset_names[i]),'rb') as F:\n",
    "                dataset = pickle.load(F)\n",
    "                \n",
    "            X_train, X_test, Y_train, Y_test = dataset\n",
    "            num_batches = int(X_train.shape[0]/(BATCH_SIZE))\n",
    "        \n",
    "            batch_accuracies = []\n",
    "            batch_train_accuracies = []\n",
    "            for batch_num in range(num_batches):\n",
    "                start_pt = BATCH_SIZE * batch_num\n",
    "                end_pt = start_pt + BATCH_SIZE\n",
    "                train_X = X_train[start_pt:end_pt]\n",
    "                train_Y = Y_train[start_pt:end_pt]\n",
    "                test_X = X_test\n",
    "                test_Y = Y_test\n",
    "\n",
    "                train_X = train_X.cuda()\n",
    "                train_Y = train_Y.cuda()\n",
    "                test_X = test_X.cuda()\n",
    "                test_Y = test_Y.cuda()\n",
    "\n",
    "                outputs = model(train_X)\n",
    "                loss = loss_fn(outputs, train_Y.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_predictions = torch.argmax(model(test_X), dim=1)\n",
    "                batch_accuracy = torch.sum(batch_predictions == test_Y)/len(test_Y)\n",
    "                batch_accuracies.append(batch_accuracy.item())\n",
    "\n",
    "                batch_train_predictions = torch.argmax(model(train_X), dim=1)\n",
    "                batch_train_accuracy = torch.sum(batch_train_predictions == train_Y)/len(train_Y)\n",
    "                batch_train_accuracies.append(batch_train_accuracy.item())        \n",
    "\n",
    "        accuracy = np.mean(batch_accuracies)\n",
    "        train_accuracy = np.mean(batch_train_accuracies)\n",
    "\n",
    "        \n",
    "        if epoch%1 == 0:\n",
    "            print(\"Epoch:%s, Train Acc:%s\"%(epoch, train_accuracy))\n",
    "            print(\"Epoch:%s, Test Acc:%s\"%(epoch, accuracy))\n",
    "        \n",
    "        if epoch == 1:\n",
    "            print(\"Epoch:%s, Train Acc:%s\"%(epoch, train_accuracy))\n",
    "            print(\"Epoch:%s, Test Acc:%s\"%(epoch, accuracy))\n",
    "        \n",
    "        model.test_accuracy = accuracy\n",
    "        model.train_accuracy = train_accuracy\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "statewide-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_train_model(N_dim, dataset, name, seed = None):\n",
    "    X_train, X_test, Y_train, Y_test = dataset\n",
    "    if seed is not None:\n",
    "        model = MLP(N_dim, name, seed).cuda()\n",
    "    else:\n",
    "        model = MLP(N_dim, name).cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.dataset = dataset\n",
    "    \n",
    "    BATCH_SIZE = 64000\n",
    "    \n",
    "    for epoch in tqdm(range(100)):\n",
    "        num_batches = int(X_train.shape[0]/(BATCH_SIZE))\n",
    "        \n",
    "        batch_accuracies = []\n",
    "        batch_train_accuracies = []\n",
    "        for batch_num in range(num_batches):\n",
    "            start_pt = BATCH_SIZE * batch_num\n",
    "            end_pt = start_pt + BATCH_SIZE\n",
    "            train_X = X_train[start_pt:end_pt]\n",
    "            train_Y = Y_train[start_pt:end_pt]\n",
    "            test_X = X_test\n",
    "            test_Y = Y_test\n",
    "            \n",
    "            train_X = train_X.cuda()\n",
    "            train_Y = train_Y.cuda()\n",
    "            test_X = test_X.cuda()\n",
    "            test_Y = test_Y.cuda()\n",
    "\n",
    "            outputs = model(train_X)\n",
    "            loss = loss_fn(outputs, train_Y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_predictions = torch.argmax(model(test_X), dim=1)\n",
    "            batch_accuracy = torch.sum(batch_predictions == test_Y)/len(test_Y)\n",
    "            batch_accuracies.append(batch_accuracy.item())\n",
    "            \n",
    "            batch_train_predictions = torch.argmax(model(train_X), dim=1)\n",
    "            batch_train_accuracy = torch.sum(batch_train_predictions == train_Y)/len(train_Y)\n",
    "            batch_train_accuracies.append(batch_train_accuracy.item())        \n",
    "        \n",
    "        accuracy = np.mean(batch_accuracies)\n",
    "        train_accuracy = np.mean(batch_train_accuracies)\n",
    "\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            print(\"Epoch:%s, Train Acc:%s\"%(epoch, train_accuracy))\n",
    "            print(\"Epoch:%s, Test Acc:%s\"%(epoch, accuracy))\n",
    "        \n",
    "        if epoch == 999:\n",
    "            print(\"Epoch:%s, Train Acc:%s\"%(epoch, train_accuracy))\n",
    "            print(\"Epoch:%s, Test Acc:%s\"%(epoch, accuracy))\n",
    "        \n",
    "        model.test_accuracy = accuracy\n",
    "        model.train_accuracy = train_accuracy\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expected-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cma_objective(x_input):\n",
    "    torch_x = torch.from_numpy(x_input).unsqueeze(0).float()\n",
    "    output = CURRENT_MODEL(torch_x)\n",
    "    pred_prob = output[0][CATEGORY_NUM].item()\n",
    "    prediction = torch.argmax(output[0]).item()\n",
    "    return pred_prob, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comparable-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cma_experiment(attacked_model, num_samples = 50):\n",
    "    cma_search_output = {}\n",
    "    global CURRENT_MODEL\n",
    "    CURRENT_MODEL = attacked_model\n",
    "    print(CURRENT_MODEL.name)\n",
    "    cma_output = CMA_info(CURRENT_MODEL.name, num_samples)        \n",
    "    \n",
    "    for i in tqdm(range(num_samples)):\n",
    "        initial_pred = 1\n",
    "        while initial_pred == 1:\n",
    "            start_pos = sample_uniform(1, N_dim, test_min, test_max)\n",
    "            output = CURRENT_MODEL(start_pos)\n",
    "            initial_pred = torch.argmax(output[0]).item()\n",
    "        cma_output.starts.append(start_pos)\n",
    "        start_pos = start_pos[0]\n",
    "        \n",
    "        es = cma.CMAEvolutionStrategy(start_pos, 0.00005)\n",
    "        try:\n",
    "            es.optimize(cma_objective, verb_disp = False, iterations=1500, correct_prediction = CATEGORY_NUM)\n",
    "            adv_offspring_ids = np.where(np.array(es.predictions) != 0)\n",
    "        except:\n",
    "            adv_offspring_ids = [[]]\n",
    "        \n",
    "        if len(adv_offspring_ids[0]) > 0:\n",
    "            random_adv_offspring_id = random.choice(list(adv_offspring_ids[0]))\n",
    "            random_adv_offspring = es.prediction_settings[random_adv_offspring_id]\n",
    "            cma_output.advs.append(random_adv_offspring)        \n",
    "\n",
    "            max_val = np.max(random_adv_offspring)\n",
    "            if max_val < x1_max:\n",
    "                cma_output.in_dist_advs.append(random_adv_offspring)\n",
    "                distance = np.linalg.norm(start_pos - random_adv_offspring)\n",
    "                cma_output.distances.append(distance)\n",
    "    return cma_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "formed-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_NUM = 0\n",
    "CURRENT_MODEL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-championship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Iteration 0\n",
      "**************************************************\n",
      "Working with with 10000000\n",
      "Torch seed:18298402211819827410\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6b8a42c03b4f388d05bf8b654f68a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Train Acc:0.8157424132823944\n",
      "Epoch:0, Test Acc:0.8158160593509675\n"
     ]
    }
   ],
   "source": [
    "all_info = []\n",
    "N_dim = 20\n",
    "for iteration in range(5):\n",
    "    print('*'*50)\n",
    "    print('Iteration %s'%iteration)\n",
    "    print('*'*50)\n",
    "    \n",
    "    trained_models = {}\n",
    "    attack_output = {}\n",
    "    for dsize in [10000000]:\n",
    "        print('Working with with %s'%dsize)\n",
    "        dset = make_dataset(dsize)\n",
    "        if dsize > 64000:\n",
    "            trained_models[dsize] = batched_train_model(N_dim, dset, 'dset_size_%s'%dsize)\n",
    "        else:\n",
    "            trained_models[dsize] = train_model(N_dim, dset, 'dset_size_%s'%dsize)\n",
    "        model_to_attack = trained_models[dsize].cpu()\n",
    "        attack_output[dsize] = cma_experiment(model_to_attack, 50)\n",
    "    iter_info = [trained_models, attack_output]\n",
    "    all_info.append(iter_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(all_info):\n",
    "    dist_means = []\n",
    "    dist_stds = []\n",
    "    attack_rate_means = []\n",
    "    attack_rate_stds = []\n",
    "\n",
    "    for dset in sorted(all_info[0][1].keys()):\n",
    "        all_distances = []\n",
    "        all_attack_rates = []\n",
    "\n",
    "        for i in range(5):\n",
    "            all_distances.extend(all_info[i][1][dset].distances)        \n",
    "            num_attacks = len(all_info[i][1][dset].in_dist_advs)\n",
    "            num_total = len(all_info[i][1][dset].starts)\n",
    "            all_attack_rates.extend([num_attacks/num_total])\n",
    "\n",
    "        attack_rate_means.append(np.mean(all_attack_rates))\n",
    "        attack_rate_stds.append(np.std(all_attack_rates))\n",
    "        dist_mean = np.mean(all_distances)\n",
    "        dist_std = np.std(all_distances)\n",
    "        dist_means.append(dist_mean)\n",
    "        dist_stds.append(dist_std)\n",
    "    \n",
    "    return attack_rate_means, attack_rate_stds, dist_means, dist_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distances(all_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-engineer",
   "metadata": {},
   "source": [
    "# For special case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "answering-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "latest-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Iteration 0\n",
      "**************************************************\n",
      "Working with with 10000000\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "Torch seed:11547436084684095053\n",
      "Found 10 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f7b27df08444e09910569fd6ccc1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:0, Train Acc:0.8515162944793702\n",
      "Epoch:0, Test Acc:0.8515441751480103\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:1, Train Acc:0.9753312993049622\n",
      "Epoch:1, Test Acc:0.9755536699295044\n",
      "Epoch:1, Train Acc:0.9753312993049622\n",
      "Epoch:1, Test Acc:0.9755536699295044\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:2, Train Acc:0.9955837917327881\n",
      "Epoch:2, Test Acc:0.9954714727401733\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:3, Train Acc:0.9952550411224366\n",
      "Epoch:3, Test Acc:0.9950703763961792\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:4, Train Acc:0.9971937966346741\n",
      "Epoch:4, Test Acc:0.9969549655914307\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:5, Train Acc:0.9975112962722779\n",
      "Epoch:5, Test Acc:0.9972968649864197\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:6, Train Acc:0.9987406754493713\n",
      "Epoch:6, Test Acc:0.998652675151825\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:7, Train Acc:0.9979225420951843\n",
      "Epoch:7, Test Acc:0.9980225801467896\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:8, Train Acc:0.999338173866272\n",
      "Epoch:8, Test Acc:0.9992882680892944\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:9, Train Acc:0.9992100501060486\n",
      "Epoch:9, Test Acc:0.9991396641731263\n",
      "dset_size_10000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a6dd2a13bc4be0b4c93b62b6b3ff3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Iteration 1\n",
      "**************************************************\n",
      "Working with with 10000000\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "sampling chunk...\n",
      "writing chunk...\n",
      "Torch seed:56586205995718371\n",
      "Found 10 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8ce0f8df774bd990f840865dfbeda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:0, Train Acc:0.8696231627464295\n",
      "Epoch:0, Test Acc:0.8692447781562805\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:1, Train Acc:0.996445677280426\n",
      "Epoch:1, Test Acc:0.996498875617981\n",
      "Epoch:1, Train Acc:0.996445677280426\n",
      "Epoch:1, Test Acc:0.996498875617981\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:2, Train Acc:0.9941669249534607\n",
      "Epoch:2, Test Acc:0.9939731764793396\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:3, Train Acc:0.9969250440597535\n",
      "Epoch:3, Test Acc:0.996960973739624\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:4, Train Acc:0.9990706729888916\n",
      "Epoch:4, Test Acc:0.9990374732017517\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:5, Train Acc:0.9993381667137146\n",
      "Epoch:5, Test Acc:0.999313063621521\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Epoch:6, Train Acc:0.9989538025856018\n",
      "Epoch:6, Test Acc:0.9988887763023376\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n",
      "Loading chunk...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-34c8fb53f157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Working with with %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunked_make_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_train_model_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chunked_datasets/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dset_size_%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmodel_to_attack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mattack_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcma_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-49a1c0e04736>\u001b[0m in \u001b[0;36mbatched_train_model_from_disk\u001b[0;34m(N_dim, dataset_chunk_folder, name, seed)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading chunk...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_chunk_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunked_dataset_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om5/user/smadan/miniconda3/envs/diff_rendering_ml/lib/python3.8/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_info = []\n",
    "N_dim = 20\n",
    "for iteration in range(5):\n",
    "    print('*'*50)\n",
    "    print('Iteration %s'%iteration)\n",
    "    print('*'*50)\n",
    "    \n",
    "    trained_models = {}\n",
    "    attack_output = {}\n",
    "    for dsize in [10000000]:\n",
    "        print('Working with with %s'%dsize)\n",
    "        dset = chunked_make_dataset(dsize, 10)\n",
    "        trained_models[dsize] = batched_train_model_from_disk(N_dim, 'chunked_datasets/', 'dset_size_%s'%dsize)\n",
    "        model_to_attack = trained_models[dsize].cpu()\n",
    "        attack_output[dsize] = cma_experiment(model_to_attack, 50)\n",
    "    iter_info = [trained_models, attack_output]\n",
    "    all_info.append(iter_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_size_experiment_5_repeats_500_dim_1000.p','wb') as F:\n",
    "#     pickle.dump(all_info, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_size_experiment_5_repeats_4_dim_standardized.p','wb') as F:\n",
    "#     pickle.dump(all_info, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_size_experiment_5_repeats_20_dim_standardized.p','wb') as F:\n",
    "#     pickle.dump(all_info, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-response",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

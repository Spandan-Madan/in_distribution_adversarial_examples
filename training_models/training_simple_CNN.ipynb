{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Purpose\n",
    "\n",
    "Setting up scripts to train CNN. This was saved as a python file and then used for launching training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/om5/user/smadan/training_scaffold_own/res/loader/multi_attribute_loader.py\n",
      "/om5/user/smadan/training_scaffold_own/res/loader\n",
      "/om5/user/smadan/training_scaffold_own/res/loader/loader.py\n",
      "/om5/user/smadan/training_scaffold_own/res/loader\n",
      "Random crops enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om5/user/smadan/miniconda3/envs/diff_rendering_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:886: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n",
      "/om5/user/smadan/miniconda3/envs/diff_rendering_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function, division\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import antialiased_cnns\n",
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "sys.path.append('/om5/user/smadan/training_scaffold_own/res/')\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import wandb\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--num_epochs', type = int, default = 50)\n",
    "# parser.add_argument('--model_arch', type = str, default = 'resnet18')\n",
    "# parser.add_argument('--batch_size', type = int, default = 100)\n",
    "# parser.add_argument('--num_classes', type = int, default = 11)\n",
    "# parser.add_argument('--base_lr', type = float, default = 0.001)\n",
    "# parser.add_argument('--use_gpu', type = bool, default = True)\n",
    "# parser.add_argument('--run_name', type = str)\n",
    "# parser.add_argument('--pretrained', action='store_true')\n",
    "# parser.add_argument('--image_size', type = int, default = 224)\n",
    "# parser.add_argument('--dataset_name', type = str, required = True)\n",
    "# parser.add_argument('--normalize', action = 'store_true')\n",
    "# parser.add_argument('--random_crop', action = 'store_true')\n",
    "# parser.add_argument('--freeze', action = 'store_true')\n",
    "# parser.add_argument('--lr_decay_step', type = int, default = -1)\n",
    "# args = parser.parse_args()\n",
    "wandb_config = {}\n",
    "wandb_config['num_epochs'] = 50\n",
    "wandb_config['model_arch'] = 'TRULY_SHIFT_INVARIANT'\n",
    "wandb_config['batch_size'] = 100\n",
    "wandb_config['num_classes'] = 11\n",
    "wandb_config['base_lr'] = 0.001\n",
    "wandb_config['use_gpu'] = True\n",
    "wandb_config['run_name'] = 'normalization_test'\n",
    "wandb_config['pretrained'] = False\n",
    "wandb_config['image_size'] = 224\n",
    "# wandb_config['dataset_name'] = 'image_train_v7_shapenet'\n",
    "wandb_config['dataset_name'] = 'image_captured_data_completely_randomized_5000'\n",
    "wandb_config['normalize'] = True\n",
    "wandb_config['random_crop'] = True\n",
    "\n",
    "\n",
    "# wandb.login()\n",
    "\n",
    "# wandb_config = dict(vars(args))\n",
    "# wandb.login()\n",
    "# NUM_EPOCHS = args.num_epochs\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# MODEL_ARCH = args.model_arch\n",
    "# NUM_CLASSES = args.num_classes\n",
    "# BASE_LR = args.base_lr\n",
    "# USE_GPU = args.use_gpu\n",
    "# SAVE_PATH = args.save_path\n",
    "# LOG_FILE = args.log_file\n",
    "# DATASET_NAME = args.dataset_name\n",
    "if wandb_config['random_crop'] == True:\n",
    "    print('Random crops enabled')\n",
    "\n",
    "# LOG_FILE_HANDLE = open(LOG_FILE, 'w')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "input_img_size = wandb_config['image_size']\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "imagenet_trainsforms = {}\n",
    "imagenet_trainsforms['train'] = transforms.Compose([\n",
    "    transforms.RandomSizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "imagenet_trainsforms['test'] = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "transforms_without_crop = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "transforms_with_crop = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "    ])\n",
    "\n",
    "\n",
    "if wandb_config['dataset_name'] == 'imagenet':\n",
    "    image_transform = {}\n",
    "    image_transform['train'] = imagenet_trainsforms['train']\n",
    "    image_transform['test'] = imagenet_trainsforms['test']\n",
    "else:\n",
    "    image_transform = {}\n",
    "    image_transform['train'] = transforms_without_crop\n",
    "    image_transform['test'] = transforms_without_crop\n",
    "\n",
    "#### Below was implemented for LeNet, leave commented ####\n",
    "# image_transform = transforms.Compose([\n",
    "#     transforms.Resize((input_img_size,input_img_size)),\n",
    "#     transforms.Grayscale(),\n",
    "#     transforms.ToTensor()\n",
    "# #     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "if wandb_config['dataset_name'] == 'imagenet':\n",
    "    loader_new = get_loader('multi_attribute_loader_file_list_imagenet')\n",
    "else:\n",
    "    loader_new = get_loader('multi_attribute_loader_file_list_shapenet')\n",
    "file_list_root = '/om5/user/smadan/dataset_lists_openmind'\n",
    "att_path = '/om5/user/smadan/differentiable_graphics_ml/training_models/shapenet_id_to_class_num.p'\n",
    "shuffles = {'train':True,'val':True,'test':False}\n",
    "\n",
    "# if wandb_config['use_gpu']:\n",
    "#     torch.cuda.set_device(0)\n",
    "\n",
    "count=0\n",
    "\n",
    "file_lists = {}\n",
    "dsets = {}\n",
    "dset_loaders = {}\n",
    "dset_sizes = {}\n",
    "for phase in ['train','test']:\n",
    "    file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,wandb_config['dataset_name'])\n",
    "    dsets[phase] = loader_new(file_lists[phase],att_path, image_transform[phase])\n",
    "    dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=wandb_config['batch_size'], shuffle = shuffles[phase], num_workers=2,drop_last=True)\n",
    "    dset_sizes[phase] = len(dsets[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "model_name = 'vit_base_patch16_224'\n",
    "model = timm.create_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head = nn.Linear(768, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_shift_batch(arr, w_shift, h_shift):\n",
    "    empty_arr = torch.zeros((arr.shape))\n",
    "    \n",
    "    if w_shift <0 and h_shift<0:\n",
    "        empty_arr[:,:,:224+h_shift,:224+w_shift] = arr[:,:,-h_shift:,-w_shift:]\n",
    "    elif w_shift >=0 and h_shift >=0:\n",
    "        empty_arr[:,:,h_shift:,w_shift:] = arr[:,:,:arr.shape[2]-h_shift,:arr.shape[3]-w_shift]\n",
    "    elif w_shift >=0 and h_shift <0:\n",
    "        empty_arr[:,:,:224+h_shift,w_shift:] = arr[:,:,-h_shift:,:arr.shape[3]-w_shift]\n",
    "    elif w_shift <0 and h_shift >=0:\n",
    "        empty_arr[:,:,h_shift:,:224+w_shift] = arr[:,:,:arr.shape[2]-h_shift,-w_shift:]\n",
    "    \n",
    "    return empty_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained = False)\n",
    "model = models.resnet18(pretrained = wandb_config['pretrained'])\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, wandb_config['num_classes'])\n",
    "model = model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = wandb_config['base_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_config['use_gpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:16<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(dset_loaders['train']):\n",
    "    inputs, labels, image_paths = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 3, 2, 1, 1, 3, 3, 1, 0, 3, 1,\n",
       "        3, 1, 0, 1, 2, 1, 0, 2, 3, 1, 1, 3, 1, 2, 2, 0, 3, 2, 2, 2, 2, 2, 3, 3,\n",
       "        0, 1, 0, 3, 1, 0, 2, 1, 2, 3, 3, 3, 3, 0, 3, 3, 1, 0, 1, 3, 3, 1, 3, 3,\n",
       "        2, 2, 1, 0, 1, 2, 1, 1, 0, 2, 0, 2, 1, 0, 3, 3, 2, 3, 2, 3, 1, 3, 3, 2,\n",
       "        0, 2, 2, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/905 [00:01<14:49,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(dset_loaders['train']):\n",
    "    inputs, labels, image_paths = data\n",
    "    inputs_orig = inputs.clone()\n",
    "    if wandb_config['use_gpu']:\n",
    "        inputs = inputs.float().cuda()\n",
    "        labels = labels.long().cuda()\n",
    "    else:\n",
    "        print('WARNING: NOT USING GPU!')\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.long()\n",
    "        \n",
    "    if wandb_config['random_crop'] == True:\n",
    "        random_shift_w = random.randint(-4,4)\n",
    "        random_shift_h = random.randint(-4,4)\n",
    "        inputs = input_shift_batch(inputs, random_shift_w, random_shift_h)\n",
    "    if wandb_config['use_gpu']:\n",
    "        inputs = inputs.float().cuda()\n",
    "    \n",
    "    if wandb_config['normalize'] == True:\n",
    "        im_means = torch.mean(inputs.view(inputs.shape[0], -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        im_stds = torch.std(inputs.view(inputs.shape[0], -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        inputs = torch.nan_to_num((inputs - im_means)/im_stds)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "#     _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if torch.isnan(torch.sum(inputs[i])):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2acce57b2a60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inputs_orig[i].detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(87.0627)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(inputs_orig[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, best_acc, best_model, configs):\n",
    "    if configs.normalize == True:\n",
    "        print('Images will be normalized')\n",
    "    model.eval()\n",
    "\n",
    "    running_corrects = 0\n",
    "    iters = 0\n",
    "    for data in tqdm(dset_loaders['test']):\n",
    "        inputs, labels, image_paths = data\n",
    "        if configs.use_gpu:\n",
    "            inputs = inputs.float().cuda()\n",
    "            labels = labels.long().cuda()\n",
    "        else:\n",
    "            print('WARNING: NOT USING GPU!')\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "\n",
    "\n",
    "        if configs.normalize == True:\n",
    "            im_means = torch.mean(inputs.view(configs.batch_size, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            im_stds = torch.std(inputs.view(configs.batch_size, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            inputs = (inputs - im_means)/im_stds\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        iters += 1\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        wandb.log({\"train_running_corrects\": running_corrects/float(iters*len(labels.data))})\n",
    "\n",
    "\n",
    "    epoch_acc = float(running_corrects) / float(dset_sizes['test'])\n",
    "\n",
    "    wandb.log({\"test_accuracy\": epoch_acc})\n",
    "\n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    wandb.log({\"best_accuracy\": best_acc})\n",
    "\n",
    "    save_path = '/om5/user/smadan/differentiable_graphics_ml/training_models/saved_models/%s.pt'%configs.run_name\n",
    "    with open(save_path,'wb') as F:\n",
    "        torch.save(best_model, F)\n",
    "\n",
    "    return best_acc, best_model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, configs):\n",
    "    if configs.normalize == True:\n",
    "        print('Images will be normalized')\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    iters = 0\n",
    "\n",
    "    for data in tqdm(dset_loaders['train']):\n",
    "        inputs, labels, image_paths = data\n",
    "        if configs.use_gpu:\n",
    "            inputs = inputs.float().cuda()\n",
    "            labels = labels.long().cuda()\n",
    "        else:\n",
    "            print('WARNING: NOT USING GPU!')\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "        if configs.random_crop == True:\n",
    "            random_shift_w = random.randint(-4,4)\n",
    "            random_shift_h = random.randint(-4,4)\n",
    "            inputs = input_shift_batch(inputs, random_shift_w, random_shift_h)\n",
    "            \n",
    "        if configs.normalize == True:\n",
    "            im_means = torch.mean(inputs.view(configs.batch_size, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            im_stds = torch.std(inputs.view(configs.batch_size, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            inputs = (inputs - im_means)/im_stds\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iters += 1\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        wandb.log({\"train_running_loss\": running_loss/float(iters*len(labels.data))})\n",
    "        wandb.log({\"train_running_corrects\": running_corrects/float(iters*len(labels.data))})\n",
    "\n",
    "    epoch_loss = float(running_loss) / dset_sizes['train']\n",
    "    epoch_acc = float(running_corrects) / float(dset_sizes['train'])\n",
    "    wandb.log({\"train_accuracy\": epoch_acc})\n",
    "    wandb.log({\"train_loss\": epoch_loss})\n",
    "    return model\n",
    "\n",
    "def model_pipeline(model, criterion, optimizer, hyperparameters):\n",
    "    with wandb.init(project=\"pytorch-test\", config=hyperparameters):\n",
    "        if hyperparameters['run_name']:\n",
    "            wandb.run.name = hyperparameters['run_name']\n",
    "        config = wandb.config\n",
    "        best_model = model\n",
    "        best_acc = 0.0\n",
    "\n",
    "        print(config)\n",
    "\n",
    "        print(config.num_epochs)\n",
    "        for epoch_num in range(config.num_epochs):\n",
    "            if wandb_config['lr_decay_step'] != -1:\n",
    "                decay_factor = 10**(int(epoch_num/wandb_config['lr_decay_step']))\n",
    "                optimizer = optim.Adam(model.parameters(), lr = wandb_config['base_lr']/decay_factor)\n",
    "            wandb.log({\"Current Epoch\": epoch_num})\n",
    "            model = train_model(model, criterion, optimizer, config)\n",
    "            best_acc, best_model = test_model(model, best_acc, best_model, config)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "############# create model, criterion and optimizer ########\n",
    "print('Checking it for you - ',wandb_config['pretrained'])\n",
    "if wandb_config['model_arch'] == 'resnet18':\n",
    "    model = models.resnet18(pretrained = wandb_config['pretrained'])\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, wandb_config['num_classes'])\n",
    "\n",
    "    if wandb_config['freeze']:\n",
    "        child_count = 0\n",
    "        for child in model.children():\n",
    "            if child_count > 2 and child_count < 9:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            child_count += 1\n",
    "elif wandb_config['model_arch'] == 'resnet18_antialiased':\n",
    "    model = antialiased_cnns.resnet18(pretrained = False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, wandb_config['num_classes'])\n",
    "elif wandb_config['model_arch'] == 'resnet50':\n",
    "    model = models.resnet50(pretrained=wandb_config['pretrained'])\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, wandb_config['num_classes'])\n",
    "elif wandb_config['model_arch'] == 'resnet152':\n",
    "    model = models.resnet152(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, wandb_config['num_classes'])\n",
    "elif wandb_config['model_arch'] == 'densenet':\n",
    "    model = models.densenet121(pretrained = False, num_classes = wandb_config['num_classes'])\n",
    "elif wandb_config['model_arch'] == 'alexnet':\n",
    "    model = models.alexnet(pretrained=wandb_config['pretrained'])\n",
    "    num_ftrs = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(num_ftrs, wandb_config['num_classes'])\n",
    "elif wandb_config['model_arch'] == 'alexnet_less_wide':\n",
    "    model = models.alexnet_less_wide(pretrained=wandb_config['pretrained'])\n",
    "    num_ftrs = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(num_ftrs, wandb_config['num_classes'])\n",
    "elif wandb_config['model_arch'] == 'simple_cnn':\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3,64,1)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.AvgPool2d(4)),\n",
    "            ('conv2', nn.Conv2d(64,32,1)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('pool2', nn.AvgPool2d(16)),\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('fc', nn.Linear(288,wandb_config['num_classes'])),\n",
    "        ]))\n",
    "else:\n",
    "    model = get_model(wandb_config['model_arch'],wandb_config['num_classes'])\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr = wandb_config['base_lr'])\n",
    "\n",
    "if wandb_config['use_gpu']:\n",
    "    criterion.cuda()\n",
    "    model.cuda()\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "\n",
    "\n",
    "best_final_model = model_pipeline(model, criterion, optimizer_ft, wandb_config)\n",
    "\n",
    "save_path = '/om5/user/smadan/differentiable_graphics_ml/training_models/saved_models/%s_final.pt'%wandb_config['run_name']\n",
    "\n",
    "with open(save_path,'wb') as F:\n",
    "    torch.save(best_final_model,F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf51de6d9239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwandb_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb_config = {}\n",
    "wandb_config['num_epochs'] = 50\n",
    "wandb_config['model_arch'] = 'TRULY_SHIFT_INVARIANT'\n",
    "wandb_config['batch_size'] = 100\n",
    "wandb_config['num_classes'] = 11\n",
    "wandb_config['base_lr'] = 0.001\n",
    "wandb_config['use_gpu'] = True\n",
    "wandb_config['run_name'] = 'normalization_test'\n",
    "wandb_config['pretrained'] = False\n",
    "wandb_config['image_size'] = 224\n",
    "wandb_config['dataset_name'] = 'imagenet'\n",
    "wandb_config['normalize'] = True\n",
    "\n",
    "wandb.login()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

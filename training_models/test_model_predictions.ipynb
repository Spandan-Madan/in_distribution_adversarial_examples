{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "machine_path = os.getcwd()\n",
    "user_root_dir = '/'.join(machine_path.split('/')[:-2])\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "sys.path.append('%s/training_scaffold_own/res/'%user_root_dir)\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Learning rate parameters\n",
    "BASE_LR = 0.001\n",
    "EPOCH_DECAY = 30 # number of epochs after which the Learning rate is decayed exponentially.\n",
    "DECAY_WEIGHT = 0.1 # factor by which the learning rate is reduced.\n",
    "\n",
    "# DATASET INFO\n",
    "NUM_CLASSES = 2 # set the number of classes in your dataset\n",
    "DATA_DIR = '../data/' # to run with the sample dataset, just set to 'hymenoptera_data'\n",
    "\n",
    "# DATALOADER PROPERTIES\n",
    "BATCH_SIZE = 10 # Set as high as possible. If you keep it too high, you'll get an out of memory error.\n",
    "\n",
    "\n",
    "### GPU SETTINGS\n",
    "CUDA_DEVICE = 0 # Enter device ID of your gpu if you want to run on gpu. Otherwise neglect.\n",
    "GPU_MODE = 1 # set to 1 if want to run on gpu.\n",
    "\n",
    "MODEL_ARCH = 'transformer'\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_TRASNFORM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_NAME = 'image_train_v3_shapenet_fixed_lighting_10_models'\n",
    "DATASET_NAME = 'image_train_v7_shapenet_40'\n",
    "# DATASET_NAME = 'image_train_v7_less_radius'\n",
    "# DATASET_NAME = 'image_captured_data_unseen_3d_model_5000'\n",
    "# DATASET_NAME = 'image_train_v3_symmetric_bias_6_conditions_camera'\n",
    "# DATASET_NAME = 'image_train_v5_shapenet_zero_removed'\n",
    "# DATASET_NAME = 'image_train_v7_shapenet_unseen_10'\n",
    "# DATASET_NAME = 'image_test_v7_cma_resnet18_v7'\n",
    "# DATASET_NAME = 'image_train_v7_clustered'\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 75\n",
    "\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "# imagenet_trainsforms = {}\n",
    "# imagenet_trainsforms['train'] = transforms.Compose([\n",
    "#     transforms.RandomSizedCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     normalize,\n",
    "# ])\n",
    "\n",
    "# imagenet_trainsforms['test'] = transforms.Compose([\n",
    "#     transforms.Scale(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     normalize,\n",
    "# ])\n",
    "\n",
    "transforms_without_crop = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "transforms_with_crop = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "    ])\n",
    "\n",
    "if DATASET_NAME == 'imagenet' or IMAGENET_TRASNFORM:\n",
    "    image_transform = {}\n",
    "    image_transform['train'] = imagenet_trainsforms['train']\n",
    "    image_transform['test'] = imagenet_trainsforms['test']\n",
    "    image_transform['leftover'] = imagenet_trainsforms['test']\n",
    "    image_transform['leftover_train'] = imagenet_trainsforms['test']\n",
    "else:\n",
    "    image_transform = {}\n",
    "    image_transform['train'] = transforms_without_crop\n",
    "    image_transform['test'] = transforms_without_crop\n",
    "    image_transform['leftover'] = transforms_without_crop\n",
    "GPU = 1\n",
    "\n",
    "NUM_CLASSES = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_new = get_loader('multi_attribute_loader_file_list_shapenet')\n",
    "if 'smadan' in user_root_dir:\n",
    "    file_list_root = '%s/dataset_lists_openmind'%user_root_dir\n",
    "elif 'spandan' in user_root_dir:\n",
    "    file_list_root = \"%s/dataset_lists_fasrc\"%user_root_dir\n",
    "\n",
    "\n",
    "att_path = '%s/differentiable_graphics_ml/training_models/shapenet_id_to_class_num.p'%user_root_dir\n",
    "shuffles = {'train':True,'val':True,'test':True,'seen_test':False,'leftover':False,'leftover_train': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.isfile(file_lists['leftover'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ GET FROM USER CONFIG - TODO #####################\n",
    "file_lists = {}\n",
    "dsets = {}\n",
    "dset_loaders = {}\n",
    "dset_sizes = {}\n",
    "for phase in ['test']:\n",
    "    file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,DATASET_NAME)\n",
    "    dsets[phase] = loader_new(file_lists[phase],att_path, image_transform[phase])\n",
    "    dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=BATCH_SIZE, shuffle = shuffles[phase], num_workers=0,drop_last=True)\n",
    "    dset_sizes[phase] = len(dsets[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fine_tuned_best_model_r18_1ep.pt',\n",
       " 'simple_cnn_fine_tuned.pt',\n",
       " 'simple_cnn_fine_tuned_2.pt',\n",
       " 'alexnet.pt',\n",
       " 'wandb_test.pt',\n",
       " 'alexnet_20_models.pt',\n",
       " 'resnet_20_models.pt',\n",
       " 'resnet_20_models_pretrained.pt',\n",
       " 'resnet_10_models.pt',\n",
       " 'resnet_10_models_0001.pt',\n",
       " 'resnet_10_models_00001.pt',\n",
       " 'resnet_10_models_000001.pt',\n",
       " 'resnet_10_models_00001_2',\n",
       " 'resnet_10_models_00001_3.pt',\n",
       " 'resnet_10_models_00001_3_final.pt',\n",
       " 'alexnet_10_models_00001_3.pt',\n",
       " 'simple_cnn_10_models_00001_3.pt',\n",
       " 'alexnet_10_models_00001_3_final.pt',\n",
       " 'simple_cnn_10_models_00001_3_final.pt',\n",
       " 'lenet_shapenet.pt',\n",
       " 'lenet_shapenet_200_epochs.pt',\n",
       " 'lenet_shapenet_final.pt',\n",
       " 'lenet_shapenet_200_epochs_final.pt',\n",
       " 'TEST_1.pt',\n",
       " 'alexnet_less_wide_10_models_00001.pt',\n",
       " 'alexnet_less_wide_10_models_00001_final.pt',\n",
       " 'alexnet_least_wide_10_models.pt',\n",
       " 'alexnet_least_wide_10_models_final.pt',\n",
       " 'alexnet_least_wide_train_v3.pt',\n",
       " 'alexnet_least_wide_train_v3_final.pt',\n",
       " 'resnet_train_v3.pt',\n",
       " 'densenet_train_v3.pt',\n",
       " 'resnet_train_v3_final.pt',\n",
       " 'densenet_train_v3_final.pt',\n",
       " 'resnet50_train_v3.pt',\n",
       " 'transformer_train_v3.pt',\n",
       " 'transformer_train_v3_final.pt',\n",
       " 'transformer_train_v3_long.pt',\n",
       " 'resnet18_train_v4.pt',\n",
       " 'transformer_train_v3_long_final.pt',\n",
       " 'resnet18_train_v4_final.pt',\n",
       " 'resnet50_train_v3_final.pt',\n",
       " 'resnet18_train_v3_fixed_camera.pt',\n",
       " 'alexnet_least_wide_train_v3_fixed_camera.pt',\n",
       " 'alexnet_least_wide_train_v3_fixed_lighting.pt',\n",
       " 'resnet18_train_v3_fixed_lighting.pt',\n",
       " 'alexnet_least_wide_train_v3_fixed_camera_final.pt',\n",
       " 'alexnet_least_wide_train_v3_fixed_lighting_final.pt',\n",
       " 'resnet18_train_v3_fixed_lighting_final.pt',\n",
       " 'resnet18_train_v3_fixed_camera_final.pt',\n",
       " 'resnet18_train_v3.pt',\n",
       " 'resnet18_train_v3_final.pt',\n",
       " 'transformer_train_v3_fixed_camera.pt',\n",
       " 'transformer_train_v3_fixed_lighting.pt',\n",
       " 'transformer_train_v3_fixed_camera_final.pt',\n",
       " 'transformer_train_v3_fixed_lighting_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions_camera.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions_camera.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_6_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_4_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_6_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_4_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_6_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_8_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_4_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_4_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_6_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_8_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_categories_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_6_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_8_conditions_camera.pt',\n",
       " 'resnet18_v5.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_categories_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_4_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_8_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_6_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_4_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_4_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_6_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_8_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_categories_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_categories_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_6_conditions_camera_final.pt',\n",
       " 'transformer_v5.pt',\n",
       " 'transformer_v5_final.pt',\n",
       " 'resnet18_v5_final.pt',\n",
       " 'densenet_v5.pt',\n",
       " 'densenet_v5_final.pt',\n",
       " 'resnet50_v5.pt',\n",
       " 'resnet50_v5_final.pt',\n",
       " 'resnet18_antialiased_v5.pt',\n",
       " 'densenet_low_lr.pt',\n",
       " 'resnet18_v5_repeat.pt',\n",
       " 'resnet18_v5_repeat_final.pt',\n",
       " 'resnet18_antialiased_v5_final.pt',\n",
       " 'densenet_low_lr_final.pt',\n",
       " 'densenet_normalized.pt',\n",
       " 'resnet_v6.pt',\n",
       " 'resnet_v6_normalized.pt',\n",
       " 'resnet_v6_final.pt',\n",
       " 'resnet_v6_normalized_final.pt',\n",
       " 'resnet_v6_normalized_01.pt',\n",
       " 'resnet_v6_normalized_000001.pt',\n",
       " 'resnet18_v6_normalized.pt',\n",
       " 'resnet18_v5_normalized.pt',\n",
       " 'resnet18_v5_normalized_final.pt',\n",
       " 'resnet18_v5_zero_removed_normalized.pt',\n",
       " 'resnet18_v5_zero_removed.pt',\n",
       " 'resnet18_v7_normalized.pt',\n",
       " 'resnet18_v6_normalized_final.pt',\n",
       " 'densenet_v7_normalized.pt',\n",
       " 'resnet18_v7_normalized_final.pt',\n",
       " 'densenet_v7_normalized_final.pt',\n",
       " 'resnet18_antialiased_v7_normalized.pt',\n",
       " 'resnet18_antialiased_v7_normalized_final.pt',\n",
       " 'resnet18_random_crops_v7_normalized.pt',\n",
       " 'resnet18_pretrained_v7_normalized.pt',\n",
       " 'resnet18_pretrained_v7_normalized_final.pt',\n",
       " 'resnet18_v7_fine_tuned.pt',\n",
       " 'resnet18_v7_fine_tuned_final.pt',\n",
       " 'resnet18_v7_fine_tuned_less_frozen.pt',\n",
       " 'resnet18_v7_fine_tuned_less_frozen_final.pt',\n",
       " 'transformer_v7_normalized.pt',\n",
       " 'transformer_v7_normalized_final.pt',\n",
       " 'resnet18_v7_subsampled_normalized.pt',\n",
       " 'resnet18_v7_subsampled_normalized_final.pt',\n",
       " 'resnet18_v7_truly_shift_invariant_normalized.pt',\n",
       " 'resnet18_v7_truly_shift_invariant_normalized_final.pt',\n",
       " 'resnet18_imagenet.pt',\n",
       " 'resnet18_v7_40.pt',\n",
       " 'resnet18_v7_40_final.pt',\n",
       " 'resnet18_imagenet_2.pt',\n",
       " 'resnet18_v7_places_normalized.pt',\n",
       " 'resnet18_v7_tfrecords.pt',\n",
       " 'resnet18_train_v7_clustered_normalized.pt',\n",
       " 'resnet18_train_v7_clustered_normalized_final.pt',\n",
       " 'resnet18_train_v7_controlled_cluster.pt',\n",
       " 'resnet18_train_v7_controlled_cluster_final.pt',\n",
       " 'resnet18_train_v7_less_radius.pt',\n",
       " 'resnet18_train_v7_shapenet_random_crop.pt',\n",
       " 'resnet18_train_v7_less_radius_final.pt',\n",
       " 'resnet18_train_v7_shapenet_random_crop_final.pt',\n",
       " 'resnet18_captured_data_randomized_5000.pt',\n",
       " 'resnet18_captured_data_unseen_3d_model_5000.pt',\n",
       " 'resnet18_captured_data_unseen_3d_model_5000_final.pt',\n",
       " 'resnet18_captured_data_unseen_3d_model_5000_normalized.pt',\n",
       " 'resnet18_captured_data_unseen_3d_model_5000_normalized_final.pt',\n",
       " 'train_v7_transformer_2.pt',\n",
       " 'train_v7_transformer_2_final.pt',\n",
       " 'train_v7_transformer_2_hybrid.pt',\n",
       " 'train_v7_transformer_2_in_21k.pt',\n",
       " 'train_v7_transformer_2_deit.pt',\n",
       " 'train_v7_transformer_40_2_hybrid.pt',\n",
       " 'train_v7_transformer_40_2.pt',\n",
       " 'train_v7_transformer_40_2_deit_distilled.pt',\n",
       " 'train_v7_transformer_2_deit_distilled.pt',\n",
       " 'train_v7_transformer_2_hybrid_final.pt',\n",
       " 'train_v7_transformer_40_2_deit.pt',\n",
       " 'train_v7_transformer_2_deit_distilled_final.pt',\n",
       " 'resnet18_pretrained_v7_40_normalized.pt',\n",
       " 'resnet18_antialiased_v7_40_normalized.pt',\n",
       " 'resnet18_v7_40_truly_shift_invariant_normalized.pt',\n",
       " 'resnet18_pretrained_v7_40_normalized_final.pt',\n",
       " 'resnet18_antialiased_v7_40_normalized_final.pt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../training_models/saved_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '../training_models/saved_models/resnet18_train_v3_fixed_lighting.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_normalized.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_train_v7_controlled_cluster_final.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_captured_data_unseen_3d_model_5000_final.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v3_symmetric_bias_6_conditions_camera.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_antialiased_v7_40_normalized.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_4'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_fine_tuned_less_frozen.pt'\n",
    "# model_path = '../training_models/saved_models/resnet18_v7_subsampled_normalized_final.pt'\n",
    "model_path = '../training_models/saved_models/resnet18_v7_40_truly_shift_invariant_normalized.pt'\n",
    "\n",
    "loaded_model = torch.load(model_path)\n",
    "loaded_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(loaded_model, torch.nn.DataParallel):\n",
    "    loaded_model = loaded_model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_path.split('/')[-1].split('.p')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resnet18_v7_40_truly_shift_invariant_normalized'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [04:23<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "all_corrects = 0\n",
    "total = 0\n",
    "incorrect_predicted_images = []\n",
    "all_paths = []\n",
    "for phase in ['test']:\n",
    "    for data in tqdm(dset_loaders[phase]):\n",
    "        inputs, labels, image_paths = data\n",
    "        inputs = inputs.float().cuda()\n",
    "#         inputs = inputs.type(torch.FloatTensor).cuda()\n",
    "        labels = labels.long().cuda()\n",
    "        if 'v7' in model_path:\n",
    "            im_means = torch.mean(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            im_stds = torch.std(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            inputs = (inputs - im_means)/im_stds\n",
    "\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.argmax(outputs,dim=1)\n",
    "        corrects = torch.sum(preds == labels).item()\n",
    "        all_corrects += corrects\n",
    "        total += len(preds)\n",
    "        all_paths.extend(image_paths)\n",
    "        incorrect_predicted_images.extend([image_paths[i] for i in torch.where(preds!=labels)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7855"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incorrect_predicted_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39399"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = all_corrects/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8005079365079365"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_predictions = {}\n",
    "# model_predictions['correct'] = [i for i in all_paths if i not in incorrect_predicted_images]\n",
    "# model_predictions['incorrect'] = incorrect_predicted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model_predictions['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model_predictions['incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(model_predictions['correct'])/len(all_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/om5/user/smadan/differentiable_graphics_ml/data/train_v7_shapenet/%s_v7_test.p'%model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/om5/user/smadan/differentiable_graphics_ml/data/train_v7_shapenet/%s_v7_test.p'%model_name, 'wb') as F:\n",
    "#     pickle.dump(model_predictions, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/om5/user/smadan/differentiable_graphics_ml/data/train_v7_shapenet/resnet18_truly_shift_invariant_incorrect_predictions.p', 'wb') as F:\n",
    "    pickle.dump(incorrect_predicted_images, F)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = '/om5/user/smadan/differentiable_graphics_ml/data/train_v7_less_radius/02691156/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ims = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for im in sorted(all_ims):\n",
    "#     plt.imshow(Image.open(\"%s/%s\"%(folder, im)))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "#     count += 1\n",
    "#     if count == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

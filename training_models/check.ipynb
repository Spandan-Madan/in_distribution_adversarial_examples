{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "machine_path = os.getcwd()\n",
    "user_root_dir = '/'.join(machine_path.split('/')[:-2])\n",
    "sys.path.append('%s/training_scaffold_own/res/'%user_root_dir)\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader\n",
    "\n",
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset_name', type = str, required = True)\n",
    "# parser.add_argument('--model_file_name', type = str, required = True)\n",
    "# parser.add_argument('--batch_size', type = int, default = 100)\n",
    "# parser.add_argument('--normalize', action = 'store_true')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "DATASET_NAME = \n",
    "# DATASET_NAME = args.dataset_name\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# MODEL_FILE_NAME = args.model_file_name\n",
    "# NORMALIZE = args.normalize\n",
    "\n",
    "transforms_without_crop = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "image_transform = {}\n",
    "image_transform['train'] = transforms_without_crop\n",
    "image_transform['test'] = transforms_without_crop\n",
    "\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "loader_new = get_loader('multi_attribute_loader_file_list_shapenet')\n",
    "if 'smadan' in user_root_dir:\n",
    "    file_list_root = '%s/dataset_lists_openmind'%user_root_dir\n",
    "elif 'spandan' in user_root_dir:\n",
    "    file_list_root = \"%s/dataset_lists_fasrc\"%user_root_dir\n",
    "\n",
    "\n",
    "att_path = '%s/differentiable_graphics_ml/training_models/shapenet_id_to_class_num.p'%user_root_dir\n",
    "shuffles = {'train':True,'val':True,'test':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "historic-electronics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fine_tuned_best_model_r18_1ep.pt',\n",
       " 'simple_cnn_fine_tuned.pt',\n",
       " 'simple_cnn_fine_tuned_2.pt',\n",
       " 'alexnet.pt',\n",
       " 'wandb_test.pt',\n",
       " 'alexnet_20_models.pt',\n",
       " 'resnet_20_models.pt',\n",
       " 'resnet_20_models_pretrained.pt',\n",
       " 'resnet_10_models.pt',\n",
       " 'resnet_10_models_0001.pt',\n",
       " 'resnet_10_models_00001.pt',\n",
       " 'resnet_10_models_000001.pt',\n",
       " 'resnet_10_models_00001_2',\n",
       " 'resnet_10_models_00001_3.pt',\n",
       " 'resnet_10_models_00001_3_final.pt',\n",
       " 'alexnet_10_models_00001_3.pt',\n",
       " 'simple_cnn_10_models_00001_3.pt',\n",
       " 'alexnet_10_models_00001_3_final.pt',\n",
       " 'simple_cnn_10_models_00001_3_final.pt',\n",
       " 'lenet_shapenet.pt',\n",
       " 'lenet_shapenet_200_epochs.pt',\n",
       " 'lenet_shapenet_final.pt',\n",
       " 'lenet_shapenet_200_epochs_final.pt',\n",
       " 'TEST_1.pt',\n",
       " 'alexnet_less_wide_10_models_00001.pt',\n",
       " 'alexnet_less_wide_10_models_00001_final.pt',\n",
       " 'alexnet_least_wide_10_models.pt',\n",
       " 'alexnet_least_wide_10_models_final.pt',\n",
       " 'alexnet_least_wide_train_v3.pt',\n",
       " 'alexnet_least_wide_train_v3_final.pt',\n",
       " 'resnet_train_v3.pt',\n",
       " 'densenet_train_v3.pt',\n",
       " 'resnet_train_v3_final.pt',\n",
       " 'densenet_train_v3_final.pt',\n",
       " 'resnet50_train_v3.pt',\n",
       " 'transformer_train_v3.pt',\n",
       " 'transformer_train_v3_final.pt',\n",
       " 'transformer_train_v3_long.pt',\n",
       " 'resnet18_train_v4.pt',\n",
       " 'transformer_train_v3_long_final.pt',\n",
       " 'resnet18_train_v4_final.pt',\n",
       " 'resnet50_train_v3_final.pt',\n",
       " 'resnet18_train_v3_fixed_camera.pt',\n",
       " 'alexnet_least_wide_train_v3_fixed_camera.pt',\n",
       " 'alexnet_least_wide_train_v3_fixed_lighting.pt',\n",
       " 'resnet18_train_v3_fixed_lighting.pt',\n",
       " 'alexnet_least_wide_train_v3_fixed_camera_final.pt',\n",
       " 'alexnet_least_wide_train_v3_fixed_lighting_final.pt',\n",
       " 'resnet18_train_v3_fixed_lighting_final.pt',\n",
       " 'resnet18_train_v3_fixed_camera_final.pt',\n",
       " 'resnet18_train_v3.pt',\n",
       " 'resnet18_train_v3_final.pt',\n",
       " 'transformer_train_v3_fixed_camera.pt',\n",
       " 'transformer_train_v3_fixed_lighting.pt',\n",
       " 'transformer_train_v3_fixed_camera_final.pt',\n",
       " 'transformer_train_v3_fixed_lighting_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions_camera.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions_camera.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_6_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_4_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_symmetric_bias_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_6_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_4_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_6_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_8_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_4_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_symmetric_bias_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_symmetric_bias_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_4_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_6_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_8_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_2_categories_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_2_categories_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_categories_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_6_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_8_conditions_camera.pt',\n",
       " 'resnet18_v5.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_categories_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_4_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_8_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_4_conditions_lighting.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_6_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_6_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_4_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_8_conditions_lighting.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_4_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_6_conditions_camera.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_8_conditions_camera.pt',\n",
       " 'resnet18_v3_fixed_lighting_unsymmetric_bias_8_categories_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_6_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_categories_4_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_fixed_camera_unsymmetric_bias_8_categories_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_2_categories_6_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_4_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_8_conditions_camera_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_8_conditions_lighting_final.pt',\n",
       " 'resnet18_v3_unsymmetric_bias_8_categories_6_conditions_camera_final.pt',\n",
       " 'transformer_v5.pt',\n",
       " 'transformer_v5_final.pt',\n",
       " 'resnet18_v5_final.pt',\n",
       " 'densenet_v5.pt',\n",
       " 'densenet_v5_final.pt',\n",
       " 'resnet50_v5.pt',\n",
       " 'resnet50_v5_final.pt',\n",
       " 'resnet18_antialiased_v5.pt',\n",
       " 'densenet_low_lr.pt',\n",
       " 'resnet18_v5_repeat.pt',\n",
       " 'resnet18_v5_repeat_final.pt',\n",
       " 'resnet18_antialiased_v5_final.pt',\n",
       " 'densenet_low_lr_final.pt',\n",
       " 'densenet_normalized.pt',\n",
       " 'resnet_v6.pt',\n",
       " 'resnet_v6_normalized.pt',\n",
       " 'resnet_v6_final.pt',\n",
       " 'resnet_v6_normalized_final.pt',\n",
       " 'resnet_v6_normalized_01.pt',\n",
       " 'resnet_v6_normalized_000001.pt',\n",
       " 'resnet18_v6_normalized.pt',\n",
       " 'resnet18_v5_normalized.pt',\n",
       " 'resnet18_v5_normalized_final.pt',\n",
       " 'resnet18_v5_zero_removed_normalized.pt',\n",
       " 'resnet18_v5_zero_removed.pt',\n",
       " 'resnet18_v7_normalized.pt',\n",
       " 'resnet18_v6_normalized_final.pt',\n",
       " 'densenet_v7_normalized.pt',\n",
       " 'resnet18_v7_normalized_final.pt',\n",
       " 'densenet_v7_normalized_final.pt',\n",
       " 'resnet18_antialiased_v7_normalized.pt',\n",
       " 'resnet18_antialiased_v7_normalized_final.pt',\n",
       " 'resnet18_random_crops_v7_normalized.pt',\n",
       " 'resnet18_pretrained_v7_normalized.pt',\n",
       " 'resnet18_pretrained_v7_normalized_final.pt',\n",
       " 'resnet18_v7_fine_tuned.pt',\n",
       " 'resnet18_v7_fine_tuned_final.pt',\n",
       " 'resnet18_v7_fine_tuned_less_frozen.pt',\n",
       " 'resnet18_v7_fine_tuned_less_frozen_final.pt',\n",
       " 'transformer_v7_normalized.pt',\n",
       " 'transformer_v7_normalized_final.pt',\n",
       " 'resnet18_v7_subsampled_normalized.pt',\n",
       " 'resnet18_v7_subsampled_normalized_final.pt',\n",
       " 'resnet18_v7_truly_shift_invariant_normalized.pt',\n",
       " 'resnet18_v7_truly_shift_invariant_normalized_final.pt',\n",
       " 'resnet18_imagenet.pt',\n",
       " 'resnet18_v7_40.pt',\n",
       " 'resnet18_v7_40_final.pt',\n",
       " 'resnet18_imagenet_2.pt',\n",
       " 'resnet18_v7_places_normalized.pt',\n",
       " 'resnet18_v7_tfrecords.pt',\n",
       " 'resnet18_train_v7_clustered_normalized.pt',\n",
       " 'resnet18_train_v7_clustered_normalized_final.pt',\n",
       " 'resnet18_train_v7_controlled_cluster.pt',\n",
       " 'resnet18_train_v7_controlled_cluster_final.pt',\n",
       " 'resnet18_train_v7_less_radius.pt',\n",
       " 'resnet18_train_v7_shapenet_random_crop.pt',\n",
       " 'resnet18_train_v7_less_radius_final.pt',\n",
       " 'resnet18_train_v7_shapenet_random_crop_final.pt',\n",
       " 'resnet18_captured_data_randomized_5000.pt',\n",
       " 'resnet18_captured_data_unseen_3d_model_5000.pt',\n",
       " 'resnet18_captured_data_unseen_3d_model_5000_final.pt',\n",
       " 'resnet18_captured_data_unseen_3d_model_5000_normalized.pt',\n",
       " 'resnet18_captured_data_unseen_3d_model_5000_normalized_final.pt',\n",
       " 'train_v7_transformer_2.pt',\n",
       " 'train_v7_transformer_2_final.pt',\n",
       " 'train_v7_transformer_2_hybrid.pt',\n",
       " 'train_v7_transformer_2_in_21k.pt',\n",
       " 'train_v7_transformer_2_deit.pt',\n",
       " 'train_v7_transformer_40_2_hybrid.pt',\n",
       " 'train_v7_transformer_40_2.pt',\n",
       " 'train_v7_transformer_40_2_deit_distilled.pt',\n",
       " 'train_v7_transformer_2_deit_distilled.pt',\n",
       " 'train_v7_transformer_2_hybrid_final.pt',\n",
       " 'train_v7_transformer_40_2_deit.pt',\n",
       " 'train_v7_transformer_2_deit_distilled_final.pt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('../training_models/saved_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ GET FROM USER CONFIG - TODO #####################\n",
    "file_lists = {}\n",
    "dsets = {}\n",
    "dset_loaders = {}\n",
    "dset_sizes = {}\n",
    "for phase in ['test']:\n",
    "    file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,DATASET_NAME)\n",
    "    dsets[phase] = loader_new(file_lists[phase],att_path, image_transform[phase])\n",
    "    dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=BATCH_SIZE, shuffle = shuffles[phase], num_workers=0,drop_last=True)\n",
    "    dset_sizes[phase] = len(dsets[phase])\n",
    "    \n",
    "    \n",
    "model_path = '../training_models/saved_models/%s'%MODEL_FILE_NAME\n",
    "loaded_model = torch.load(model_path)\n",
    "loaded_model.cuda();\n",
    "if isinstance(loaded_model, torch.nn.DataParallel):\n",
    "    loaded_model = loaded_model.module\n",
    "model_name = model_path.split('/')[-1].split('.p')[0]\n",
    "\n",
    "\n",
    "all_corrects = 0\n",
    "total = 0\n",
    "incorrect_predicted_images = []\n",
    "all_paths = []\n",
    "for phase in ['test']:\n",
    "    for data in tqdm(dset_loaders[phase]):\n",
    "        inputs, labels, image_paths = data\n",
    "        inputs = inputs.float().cuda()\n",
    "        labels = labels.long().cuda()\n",
    "        if NORMALIZE:\n",
    "            im_means = torch.mean(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            im_stds = torch.std(inputs.view(BATCH_SIZE, -1),dim=1).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            inputs = (inputs - im_means)/im_stds\n",
    "\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.argmax(outputs,dim=1)\n",
    "        corrects = torch.sum(preds == labels).item()\n",
    "        all_corrects += corrects\n",
    "        total += len(preds)\n",
    "        all_paths.extend(image_paths)\n",
    "        incorrect_predicted_images.extend([image_paths[i] for i in torch.where(preds!=labels)[0]])\n",
    "\n",
    "acc = all_corrects/total\n",
    "print_string = \"%s___%s___%s\"%(DATASET_NAME, MODEL_FILE_NAME, acc)\n",
    "with open('prediction_results.txt','a') as F:\n",
    "    print(print_string, file = F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

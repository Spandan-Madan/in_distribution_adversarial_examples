{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, grad\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "machine_path = os.getcwd()\n",
    "user_root_dir = '/'.join(machine_path.split('/')[:-2])\n",
    "sys.path.insert(0,'%s/redner/'%user_root_dir)\n",
    "import pyredner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyRedner location: /net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print('PyRedner location: %s'%pyredner.__file__)\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset_name', type = str, required = True)\n",
    "# parser.add_argument('--model_files_pickle_name', type = str, required = True)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# DATASET_NAME = args.dataset_name\n",
    "# MODEL_FILES_PICKLE_NAME = args.model_files_pickle_name\n",
    "\n",
    "DATASET_NAME = \"shapenet_illumination_and_camera_model\"\n",
    "MODEL_FILES_PICKLE_NAME = \"categories_10_models_10.pkl\"\n",
    "\n",
    "SHAPENET_DIR = '%s/ShapeNetCore.v2'%user_root_dir\n",
    "\n",
    "dataset_path = \"%s/differentiable_graphics_ml/data/%s\"%(user_root_dir, DATASET_NAME)\n",
    "model_files_pickle_path = '%s/differentiable_graphics_ml/rendering/shapenet_model_subsets/%s'%(user_root_dir, MODEL_FILES_PICKLE_NAME)\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    print('This is a new dataset, creating a new folder at: %s'%dataset_path)\n",
    "    os.mkdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LIGHTS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_light_positions(pos, num_lights):\n",
    "    all_combinations = [[1,1,1],[-1,1,1],[1,-1,1],[1,1,-1],[1,-1,-1],[-1,1,-1],[-1,-1,1],[-1,-1,-1]]\n",
    "    x,y,z = pos[0],pos[1],pos[2]\n",
    "    if num_lights == 1:\n",
    "        return [pos]\n",
    "    else:\n",
    "        chosen_combs = random.sample(all_combinations, num_lights-1)\n",
    "\n",
    "        new_light_positions = [pos]\n",
    "        for comb in chosen_combs:\n",
    "            new_light_positions.append(torch.tensor([x*comb[0], y*comb[1], z*comb[2]]))\n",
    "        return new_light_positions\n",
    "\n",
    "def render_shapenet_obj(obj_path, camera_position, light_position, light_intensity, show_lights = False,render_seed = 1):\n",
    "    obj_model_all = pyredner.load_obj(obj_path, return_objects=True)\n",
    "    obj_model = [i for i in obj_model_all if len(i.vertices)>0]\n",
    "    m = pyredner.Material(diffuse_reflectance = torch.tensor((0.8, 0.8, 0.8), device = pyredner.get_device()))\n",
    "    for part in obj_model:\n",
    "        part.material = m\n",
    "\n",
    "    scene_cam = pyredner.automatic_camera_placement(obj_model, resolution = (224, 224))\n",
    "    scene_cam.position = camera_position\n",
    "    \n",
    "    all_light_positions = get_light_positions(light_position, NUM_LIGHTS)\n",
    "    scene_lights = []\n",
    "    \n",
    "    for l_pos in all_light_positions:\n",
    "        scene_light = pyredner.generate_quad_light(position = l_pos,\n",
    "                                         look_at = torch.zeros(3),\n",
    "                                         size = torch.tensor([0.5, 0.5]),\n",
    "                                         intensity = light_intensity,\n",
    "                                         directly_visible = show_lights)\n",
    "        scene_lights.append(scene_light)\n",
    "        \n",
    "    all_objects = obj_model + scene_lights\n",
    "    scene = pyredner.Scene(objects = all_objects, camera = scene_cam)\n",
    "    img = pyredner.render_pathtracing(scene,num_samples=256,seed=render_seed)\n",
    "    im = torch.pow(img.data, 1.0/2.2).cpu()\n",
    "    im = im*255/torch.max(im)\n",
    "    image = Image.fromarray(im.numpy().astype('uint8'))\n",
    "    return image, torch.sum(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def render_shapenet_obj_non_stochastic(obj_path, camera_position, all_light_positions, light_intensity, show_lights = False,render_seed = 1):\n",
    "#     obj_model_all = pyredner.load_obj(obj_path, return_objects=True)\n",
    "#     obj_model = [i for i in obj_model_all if len(i.vertices)>0]\n",
    "#     m = pyredner.Material(diffuse_reflectance = torch.tensor((0.8, 0.8, 0.8), device = pyredner.get_device()))\n",
    "#     for part in obj_model:\n",
    "#         part.material = m\n",
    "\n",
    "#     scene_cam = pyredner.automatic_camera_placement(obj_model, resolution = (512, 512))\n",
    "#     scene_cam.position = camera_position\n",
    "\n",
    "#     scene_lights = []\n",
    "\n",
    "#     for l_pos in all_light_positions:\n",
    "#         scene_light = pyredner.generate_quad_light(position = l_pos,\n",
    "#                                          look_at = torch.zeros(3),\n",
    "#                                          size = torch.tensor([0.5, 0.5]),\n",
    "#                                          intensity = light_intensity,\n",
    "#                                          directly_visible = show_lights)\n",
    "#         scene_lights.append(scene_light)\n",
    "\n",
    "#     all_objects = obj_model + scene_lights\n",
    "#     scene = pyredner.Scene(objects = all_objects, camera = scene_cam)\n",
    "#     img = pyredner.render_pathtracing(scene,num_samples=256,seed=render_seed)\n",
    "#     im = torch.pow(img.data, 1.0/2.2).cpu()\n",
    "#     im = im*255/torch.max(im)\n",
    "#     image = Image.fromarray(im.numpy().astype('uint8'))\n",
    "#     return image, im\n",
    "\n",
    "# def generate_spherical_light_points(num_steps = 10, radius = 1):\n",
    "#     xs = np.linspace(-1,1,num_steps)\n",
    "#     ys = np.linspace(-1,1,num_steps)\n",
    "#     zs = np.zeros((num_steps,num_steps))\n",
    "#     zs_2 = np.zeros((num_steps,num_steps))\n",
    "\n",
    "#     all_points = []\n",
    "\n",
    "#     for i in range(len(xs)):\n",
    "#         x_val = xs[i]\n",
    "#         for j in range(len(ys)):\n",
    "#             y_val = ys[j]\n",
    "#             z_squared = 1 - x_val**2 - y_val**2\n",
    "\n",
    "#             if z_squared < 0 :\n",
    "#                 zs[i,j] = 0\n",
    "#             else:\n",
    "#                 zs[i,j] = np.sqrt(z_squared)\n",
    "#                 zs_2[i,j] = -np.sqrt(z_squared)\n",
    "\n",
    "#                 point_1 = [radius * x_val, radius * y_val, radius * zs[i,j]]\n",
    "#                 point_2 = [radius * x_val, radius * y_val, radius * zs_2[i,j]]\n",
    "                \n",
    "# #                 if point_1[0] > 1 and point_1[1] > 1 and point_1[2] > 1:\n",
    "#                 all_points.append(point_1)\n",
    "                \n",
    "#                 if zs[i,j] !=  0.0:\n",
    "#                     all_points.append(point_2)\n",
    "                    \n",
    "#     return all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(10000):\n",
    "    points.append(np.random.normal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(points,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05935914, -0.20752042, -0.97642807])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_on_sphere(num_points, radius):\n",
    "    points = []\n",
    "    for i in range(num_points):\n",
    "        X = np.random.normal()\n",
    "        Y = np.random.normal()\n",
    "        Z = np.random.normal()\n",
    "\n",
    "        vector = np.array([X,Y,Z])\n",
    "        point = list(radius*vector/np.linalg.norm(vector))\n",
    "        points.append(point)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_points = generate_uniform_on_sphere(20,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scene setup\n",
    "# cam_pos = torch.tensor([1.0,1.0,1.0])\n",
    "light_intensity = torch.tensor([1.0,1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light_poses = [[.5, .5, -0.5], [1,1,1],[0.5,1,2],[0.5,1,0.5],[2.5,3,0.5],[3,4,0.3],[0.4,1,2.5],[4,4,4],[8,1,0.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_files_pickle_path = '/om5/user/smadan/differentiable_graphics_ml/rendering/%s'%MODEL_FILES_PICKLE\n",
    "with open(model_files_pickle_path, 'rb') as F:\n",
    "    model_files = pickle.load(F)\n",
    "\n",
    "sphere_light_poses = generate_spherical_light_points(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphere_light_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam_pos = torch.tensor([1.0,1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pos_list = [[1.0,1.0,1.0],[0.0,1.0,1.414],[-1.0,1.0,1.0],[-1.414,1.0,0.0],\n",
    "            [-1.0,1.0,-1.0],[0.0,1.0,-1.414],[1.0,1.0,-1.0],[1.414,1.0,0.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/net/coxfs01/srv/export/coxfs01/pfister_lab2/share_root/Lab/spandan/differentiable_graphics_ml/data/shapenet_illumination_and_camera_model'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/net/coxfs01/srv/export/coxfs01/pfister_lab2/share_root/Lab/spandan/differentiable_graphics_ml/data/shapenet_illumination_and_camera_model'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18859.28it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19934.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 37183.55it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 39606.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 31895.85it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 42538.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 42625.04it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 44104.14it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 43509.38it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 47074.12it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 46243.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for category in sorted(model_files.keys()):\n",
    "    category_dir = \"%s/%s\"%(SHAPENET_DIR, category)\n",
    "    instance_model_files = model_files[category]\n",
    "    for model_file in instance_model_files:\n",
    "        model_file = model_file.replace('/om5/user/smadan',user_root_dir)\n",
    "        instance = model_file.split('/')[-3]\n",
    "        for cam_pos in cam_pos_list:\n",
    "            for lp in cam_pos_list:\n",
    "                cam_pos = torch.tensor(cam_pos)\n",
    "                light_pos_list = [3*i for i in lp]\n",
    "                light_pos = torch.tensor(light_pos_list).float()\n",
    "                rendered_im, im_sum = render_shapenet_obj(model_file, cam_pos, light_pos, light_intensity, False,render_seed=1)\n",
    "                image_name = \"%s/%s_%s_%.03f_%.03f_%.03f_%.03f_%.03f_%.03f.png\"%(dataset_path, category, instance, cam_pos[0].item(), cam_pos[1].item(), cam_pos[2].item(), light_pos[0].item(), light_pos[1].item(), light_pos[2].item())\n",
    "                rendered_im.save(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category in sorted(model_files.keys()):\n",
    "#     category_dir = \"%s/%s\"%(SHAPENET_DIR, category)\n",
    "#     instance_model_files = model_files[category]\n",
    "#     for model_file in tqdm(instance_model_files):\n",
    "#         print(model_file)\n",
    "#         instance = model_file.split('/')[7]\n",
    "#         for lp in sphere_light_poses:\n",
    "#             light_pos = torch.tensor(lp).float()\n",
    "#             rendered_im, im_sum = render_shapenet_obj(model_file, cam_pos, light_pos, light_intensity, False,render_seed=1)\n",
    "#             image_name = \"%s/%s_%s_%.03f_%.03f_%.03f.png\"%(dataset_path, category, instance, light_pos[0].item(), light_pos[1].item(), light_pos[2].item())\n",
    "#             rendered_im.save(image_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

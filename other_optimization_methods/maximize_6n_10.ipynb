{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending inverse rendering to our scene parametrization in train_v6 and train_v7\n",
    "\n",
    "In these datasets we have a total of 10 camera parameters and 6xnum_lights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, Javascript\n",
    "\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/__init__.py\n",
      "/net/storage001.ib.cluster/om2/user/smadan/differentiable_graphics_ml/other_optimization_methods\n",
      "/net/storage001.ib.cluster/om2/user/smadan/training_scaffold_own/res/loader/multi_attribute_loader.py\n",
      "/net/storage001.ib.cluster/om2/user/smadan/training_scaffold_own/res/loader\n",
      "/net/storage001.ib.cluster/om2/user/smadan/training_scaffold_own/res/loader/loader.py\n",
      "/net/storage001.ib.cluster/om2/user/smadan/training_scaffold_own/res/loader\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../inverse_rendering/')\n",
    "from general_imports import *\n",
    "from tqdm import tqdm as tqdm_file\n",
    "import argparse\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import exp\n",
    "from numpy import sqrt\n",
    "from numpy import cos\n",
    "from numpy import e\n",
    "from numpy import pi\n",
    "from numpy import argsort\n",
    "from numpy.random import randn\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key is: afPtoGhPArIJ7LK6\n"
     ]
    }
   ],
   "source": [
    "alphanumeric_key = ''.join(random.choices(string.ascii_letters + string.digits, k=16))\n",
    "print(\"Key is: %s\"%alphanumeric_key)\n",
    "\n",
    "##### Load Model #####\n",
    "model_path = '../training_models/saved_models/resnet18_v7_normalized_final.pt'\n",
    "loaded_model = torch.load(model_path)\n",
    "softmax_layer = nn.Softmax()\n",
    "\n",
    "#### Load Shapenet Category etc. ####\n",
    "with open('../training_models/shapenet_class_num_to_class_name.p','rb') as F:\n",
    "    shapenet_class_num_to_class_name = pickle.load(F)\n",
    "with open('../training_models/shapenet_id_to_class_num.p','rb') as F:\n",
    "    shapenet_id_to_class_num = pickle.load(F)\n",
    "\n",
    "##### Randomization Functions ######\n",
    "\n",
    "RADIUS_MIN = 8.0\n",
    "RADIUS_MAX = 14.0\n",
    "MIN_NUM_LIGHTS = 1\n",
    "MAX_NUM_LIGHTS = 4\n",
    "\n",
    "RADIUS_MIN_CAM = 0.5\n",
    "RADIUS_MAX_CAM = 8.0\n",
    "\n",
    "def generate_uniform_on_sphere(num_points, radius):\n",
    "    points = []\n",
    "    for i in range(num_points):\n",
    "        X = np.random.normal()\n",
    "        Y = np.random.normal()\n",
    "        Z = np.random.normal()\n",
    "\n",
    "        vector = np.array([X,Y,Z])\n",
    "        point = list(radius*vector/np.linalg.norm(vector))\n",
    "        points.append(point)\n",
    "    return points\n",
    "def get_cam_position(radius_min, radius_max):\n",
    "    random_radius = random.uniform(radius_min, radius_max)\n",
    "    cam_point = generate_uniform_on_sphere(1, random_radius)[0]\n",
    "    cam_point = torch.tensor(cam_point).float()\n",
    "\n",
    "    return cam_point\n",
    "def get_positions(min_num_lights, max_num_lights, radius_min, radius_max):\n",
    "    num_lights = random.choice(range(min_num_lights, max_num_lights + 1))\n",
    "    light_positions = []\n",
    "\n",
    "    for num in range(num_lights):\n",
    "        random_radius = random.uniform(radius_min, radius_max)\n",
    "        light_point = generate_uniform_on_sphere(1, random_radius)[0]\n",
    "        light_point = torch.tensor(light_point).float()\n",
    "        light_positions.append(light_point)\n",
    "\n",
    "    return light_positions\n",
    "def get_random_intensity():\n",
    "    light_intensity = torch.tensor([random.uniform(0,1), \\\n",
    "                                    random.uniform(0,1), random.uniform(0,1)]).float()\n",
    "    return light_intensity\n",
    "def get_random_reflectance():\n",
    "    specular_reflectance = torch.tensor([random.uniform(0,1), \\\n",
    "                                    random.uniform(0,1), random.uniform(0,1)], device = pyredner.get_device()).float()\n",
    "    return specular_reflectance\n",
    "def get_random_look_at(radius):\n",
    "    K = 0.3\n",
    "    look_at = torch.tensor([random.uniform(0,K*radius), random.uniform(0,K*radius), random.uniform(0,K*radius)]).float()\n",
    "    return look_at\n",
    "def get_random_scene_params(model_file):\n",
    "    ### Random Camera Settings ###\n",
    "    camera_position = get_cam_position(RADIUS_MIN_CAM, RADIUS_MAX_CAM)\n",
    "    cam_radius = torch.sqrt(camera_position[0]**2 + camera_position[1]**2 + camera_position[2]**2).item()\n",
    "    cam_look_at = get_random_look_at(cam_radius)\n",
    "    fov = torch.tensor(random.uniform(35,100))\n",
    "    cam_up = torch.tensor([random.uniform(-1,1), random.uniform(-1,1), random.uniform(-1,1)])\n",
    "    initial_camera_params = {'camera_position': camera_position,\n",
    "                             'cam_look_at': cam_look_at,\n",
    "                             'fov': fov,\n",
    "                             'cam_up': cam_up}\n",
    "\n",
    "\n",
    "    ### Random Light Settings ###\n",
    "    all_light_positions = get_positions(MIN_NUM_LIGHTS, MAX_NUM_LIGHTS, RADIUS_MIN, RADIUS_MAX)\n",
    "    all_light_look_ats = [get_random_look_at(cam_radius) for i in all_light_positions]\n",
    "    all_light_intensities = [get_random_intensity() for i in all_light_positions]\n",
    "    all_light_sizes = [torch.tensor([random.uniform(0.1,5.0), random.uniform(0.1, 5.0)]) for i in all_light_positions]\n",
    "    initial_light_params = {'all_light_positions': all_light_positions,\n",
    "                            'all_light_look_ats': all_light_look_ats,\n",
    "                            'all_light_intensities': all_light_intensities,\n",
    "                            'all_light_sizes': all_light_sizes}\n",
    "\n",
    "    #### Material ####\n",
    "    diffuse_material = pyredner.Material(diffuse_reflectance = torch.tensor([1.0, 1.0, 1.0], \\\n",
    "                                        device='cuda:0'), two_sided = True)\n",
    "    material_settings = {'diffuse_material':diffuse_material}\n",
    "\n",
    "    scene_params = [model_file, initial_camera_params, initial_light_params, material_settings]\n",
    "    return scene_params\n",
    "def show_inputs(inp, save_path = False, title='No Title'):\n",
    "    plt.imshow(inp[0].cpu().permute(1,2,0).int())\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    if save_path != False:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "def render_input(scene):\n",
    "    img = pyredner.render_pathtracing(scene = scene, num_samples = 256, seed = 1, use_secondary_edge_sampling = False)\n",
    "    img = torch.clamp(img, min = 0.00000001)\n",
    "    img = torch.pow(img, 1.0/2.2)\n",
    "    img = img*255/torch.max(img)\n",
    "    inputs = img.permute(2,0,1).unsqueeze(0)\n",
    "    return inputs\n",
    "def spherical_to_cartesian(r,theta,phi):\n",
    "    theta_rad = theta*3.14/180\n",
    "    phi_rad = phi*3.14/180\n",
    "\n",
    "    x = r * torch.sin(phi_rad) * torch.cos(theta_rad)\n",
    "    y = r * torch.sin(phi_rad) * torch.sin(theta_rad)\n",
    "    z = r * torch.cos(phi_rad)\n",
    "    out =  torch.stack([x,y,z])\n",
    "    return out\n",
    "def load_geometry(model_file, geometry, mat):\n",
    "    if geometry:\n",
    "        obj_model_all = model_file\n",
    "        obj_model = [i for i in obj_model_all if len(i.vertices)>0]\n",
    "    else:\n",
    "        obj_model_all = pyredner.load_obj(model_file, return_objects=True)\n",
    "        obj_model = [i for i in obj_model_all if len(i.vertices)>0]\n",
    "\n",
    "    for part in obj_model:\n",
    "        part.material = mat\n",
    "\n",
    "    return obj_model\n",
    "def optimize_flags(obj):\n",
    "    if type(obj) == list:\n",
    "        for o in obj:\n",
    "            o.requires_grad = True\n",
    "    else:\n",
    "        obj.requires_grad = True\n",
    "    return obj\n",
    "def setup_scene(scene_params):\n",
    "\n",
    "    model_file, camera_params, light_params, material_settings = scene_params\n",
    "    obj_model = load_geometry(model_file, False, material_settings['diffuse_material'])\n",
    "\n",
    "    #### Camera Setup ####\n",
    "    scene_cam = pyredner.automatic_camera_placement(obj_model, resolution = (224, 224),\n",
    "                                                   fov = torch.tensor([camera_params['fov']]),\n",
    "                                                   up = camera_params['cam_up'],\n",
    "                                                   look_at = camera_params['cam_look_at'])\n",
    "    scene_cam.position = camera_params['camera_position']\n",
    "\n",
    "    #### Lights Setup ####\n",
    "    scene_lights = []\n",
    "    num_lights = len(light_params['all_light_positions'])\n",
    "    for i in range(num_lights):\n",
    "        scene_light = pyredner.generate_quad_light(position = light_params['all_light_positions'][i],\n",
    "                                     look_at = light_params['all_light_look_ats'][i],\n",
    "                                     size = light_params['all_light_sizes'][i],\n",
    "                                     intensity = light_params['all_light_intensities'][i],\n",
    "                                     directly_visible = False)\n",
    "        scene_lights.append(scene_light)\n",
    "\n",
    "    all_objects = obj_model + scene_lights\n",
    "    scene = pyredner.Scene(objects = all_objects, camera = scene_cam)\n",
    "\n",
    "    return scene\n",
    "def start_up(scene_params, optimized_params):\n",
    "    model_file, camera_params, light_params, material_settings = scene_params\n",
    "    variables = []\n",
    "    var_names_list = []\n",
    "    for param in optimized_params:\n",
    "        if param in camera_params.keys():\n",
    "            optimize_flags(camera_params[param])\n",
    "            if type(camera_params[param]) == list:\n",
    "                variables.extend(camera_params[param])\n",
    "            else:\n",
    "                variables.append(camera_params[param])\n",
    "        elif param in light_params.keys():\n",
    "            optimize_flags(light_params[param])\n",
    "            if type(light_params[param]) == list:\n",
    "                variables.extend(light_params[param])\n",
    "            else:\n",
    "                variables.append(light_params[param])\n",
    "\n",
    "    scene_params = [model_file, camera_params, light_params, material_settings]\n",
    "    scene = setup_scene(scene_params)\n",
    "\n",
    "    return scene, variables\n",
    "def get_random_3d_model():\n",
    "    model_files_pickle = '../rendering/shapenet_model_subsets/categories_10_models_10.pkl'\n",
    "    with open(model_files_pickle, 'rb') as F:\n",
    "        model_files = pickle.load(F)\n",
    "    random_category = random.choice(list(model_files.keys()))\n",
    "    random_instance = random.choice(model_files[random_category]).split('/')[7]\n",
    "    model_file = '%s/ShapeNetCore.v2/%s/%s/models/model_normalized.obj'%(user_root_dir, random_category, random_instance)\n",
    "    print('Chosen model is a %s'%shapenet_class_num_to_class_name[shapenet_id_to_class_num[random_category]])\n",
    "    category_num = shapenet_id_to_class_num[random_category]\n",
    "    print('Category num is %s'%category_num)\n",
    "    return model_file, category_num, random_category, random_instance\n",
    "def perturbation_vector(factor_len, perturb_percent = 1):\n",
    "    if factor_len == 0:\n",
    "        factor_len = 1\n",
    "    vec = torch.tensor([random.uniform(-perturb_percent/100,perturb_percent/100) for i in range(factor_len)])\n",
    "    return 1 + vec\n",
    "def random_perturbed_params(scene_params, perturb_percent = 1):\n",
    "    new_scene_params = [0,0,0,0]\n",
    "    new_scene_params[0] = scene_params[0]\n",
    "\n",
    "    new_scene_params[1] = {}\n",
    "    for key in scene_params[1].keys():\n",
    "        new_scene_params[1][key] = scene_params[1][key].clone() * perturbation_vector(scene_params[1][key].dim(), perturb_percent)\n",
    "\n",
    "    new_scene_params[2] = {}\n",
    "    for key in scene_params[2].keys():\n",
    "        new_scene_params[2][key] = [i.clone() * perturbation_vector(i.dim(), perturb_percent) for i in scene_params[2][key]]\n",
    "\n",
    "    new_scene_params[3] = scene_params[3]\n",
    "    return new_scene_params\n",
    "def render_and_predict(scene_params, optimized_params):\n",
    "    scene, variables = start_up(scene_params, optimized_params)\n",
    "    inputs = render_input(scene)\n",
    "    inputs = inputs.cuda()\n",
    "    rendered_inputs = inputs.clone()\n",
    "    inputs = inputs/255.0\n",
    "    im_means = torch.mean(inputs).unsqueeze(0).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "    im_stds = torch.std(inputs).unsqueeze(0).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "    inputs_ = (inputs - im_means)/im_stds\n",
    "    outputs = softmax_layer(loaded_model(inputs_))\n",
    "    prediction = torch.argmax(outputs).item()\n",
    "    return prediction, rendered_inputs\n",
    "def create_folder(folder):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "def clone_scene_params(scene_params):\n",
    "    original_scene_params = [0,0,0,0]\n",
    "    original_scene_params[0] = scene_params[0]\n",
    "\n",
    "    original_scene_params[1] = {}\n",
    "    for key in scene_params[1].keys():\n",
    "        original_scene_params[1][key] = scene_params[1][key].clone()\n",
    "\n",
    "    original_scene_params[2] = {}\n",
    "    for key in scene_params[2].keys():\n",
    "        original_scene_params[2][key] = [i.clone() for i in scene_params[2][key]]\n",
    "\n",
    "    original_scene_params[3] = scene_params[3]\n",
    "    return original_scene_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMA-ES based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model_file, category_num, category, instance = get_random_3d_model()\n",
    "\n",
    "# optimized_params = ['camera_position', 'cam_look_at', 'fov', 'cam_up',\n",
    "#                     'all_light_positions', 'all_light_look_ats','all_light_sizes', 'all_light_intensities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    plt.plot(losses, marker='o',color='green')\n",
    "    plt.title('Losses',fontsize=16)\n",
    "    plt.xticks(fontsize= 12)\n",
    "    plt.yticks(fontsize= 12)\n",
    "    plt.show()\n",
    "    \n",
    "def check_in_range(scene_params):\n",
    "    if scene_params[0] == 0:\n",
    "        return False\n",
    "    cam_radius = torch.norm(scene_params[1]['camera_position'])\n",
    "    radius_check = RADIUS_MIN_CAM <= cam_radius <= RADIUS_MAX_CAM\n",
    "    look_at_check = torch.sum(scene_params[1]['cam_look_at'] < cam_radius*0.3).item() == 3\n",
    "    return radius_check and look_at_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_model = nn.Sequential(*list(loaded_model.children())[:-1])\n",
    "shorter_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene_params = get_random_scene_params(model_file)\n",
    "# original_scene_params = clone_scene_params(scene_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene_params = clone_scene_params(original_scene_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../activations_brute_force/neuron_instance_to_grid_activations_resnet18_v7_test.json','r') as F:\n",
    "    acts = json.load(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_range(range_low, range_high):\n",
    "    return (range_low + rand(1) * (range_high - range_low))[0]\n",
    "\n",
    "def get_new_candidate(original_scene_params):\n",
    "    candidate_params = [0,0,0,0]\n",
    "    while check_in_range(candidate_params) == False:\n",
    "        candidate_params[0] = original_scene_params[0]      \n",
    "        candidate_params[1] = {}\n",
    "        candidate_params[1]['camera_position'] = torch.tensor([get_in_range(RADIUS_MIN_CAM, RADIUS_MAX_CAM), get_in_range(RADIUS_MIN_CAM, RADIUS_MAX_CAM), get_in_range(RADIUS_MIN_CAM, RADIUS_MAX_CAM)])\n",
    "        radius = torch.norm(candidate_params[1]['camera_position']).item()\n",
    "        candidate_params[1]['cam_look_at'] = torch.tensor([get_in_range(-0.2*radius, 0.2*radius), get_in_range(-0.2*radius, 0.2*radius), get_in_range(-0.2*radius, 0.2*radius)]).float()\n",
    "        candidate_params[1]['fov'] = original_scene_params[1]['fov']\n",
    "        candidate_params[1]['cam_up'] = torch.tensor([get_in_range(-1,1), get_in_range(-1,1), get_in_range(-1,1)]).float()\n",
    "        \n",
    "        candidate_params[2] = original_scene_params[2]\n",
    "        candidate_params[3] = original_scene_params[3]\n",
    "    return candidate_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pop_member(original_scene_params, step_size):\n",
    "    candidate_params = [0,0,0,0]\n",
    "    while check_in_range(candidate_params) == False:\n",
    "        candidate_params[0] = original_scene_params[0]      \n",
    "        candidate_params[1] = {}\n",
    "        candidate_params[1]['camera_position'] = original_scene_params[1]['camera_position'] + torch.rand(3)*step_size\n",
    "        candidate_params[1]['cam_look_at'] = original_scene_params[1]['cam_look_at'] + torch.rand(3)*step_size\n",
    "        candidate_params[1]['cam_look_at'] = candidate_params[1]['cam_look_at'].float()\n",
    "        candidate_params[1]['fov'] = original_scene_params[1]['fov']\n",
    "        candidate_params[1]['cam_up'] = original_scene_params[1]['cam_up'] + torch.rand(3)*step_size\n",
    "        candidate_params[1]['cam_up'] = candidate_params[1]['cam_up'].float()\n",
    "        candidate_params[2] = original_scene_params[2]\n",
    "        candidate_params[3] = original_scene_params[3]\n",
    "    return candidate_params\n",
    "\n",
    "def objective(scene_params, optimized_params, neuron_num = None):\n",
    "    scene, variables = start_up(scene_params, [])\n",
    "    inputs = render_input(scene)\n",
    "    inputs = inputs.cuda()\n",
    "    rendered_inputs = inputs.clone()\n",
    "    inputs = inputs/255.0\n",
    "    im_means = torch.mean(inputs).unsqueeze(0).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "    im_stds = torch.std(inputs).unsqueeze(0).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "    inputs_ = (inputs - im_means)/im_stds\n",
    "    outputs = shorter_model(inputs_)\n",
    "    if neuron_num is None:\n",
    "        selective_neurons = torch.argsort(outputs[0].view(512,-1).squeeze(1))[-1]\n",
    "        selected_act_firing = outputs[0].view(512,-1).squeeze(1)[selective_neurons]\n",
    "        return selective_neurons, selected_act_firing\n",
    "    else:\n",
    "        loss = torch.sum(outputs[0].view(512,-1).squeeze(1)[neuron_num]).item()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cma_objective_adversarial(x):\n",
    "    SCENE_PARAMS[1]['camera_position'] = torch.tensor(x[:3]).float()\n",
    "    SCENE_PARAMS[1]['cam_look_at'] = torch.tensor(x[3:6]).float()\n",
    "    \n",
    "    scene, variables = start_up(SCENE_PARAMS, [])\n",
    "    inputs = render_input(scene)\n",
    "    inputs = inputs.cuda()\n",
    "    rendered_inputs = inputs.clone()\n",
    "    inputs = inputs/255.0\n",
    "    im_means = torch.mean(inputs).unsqueeze(0).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "    im_stds = torch.std(inputs).unsqueeze(0).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "    inputs_ = (inputs - im_means)/im_stds\n",
    "    outputs = softmax_layer(loaded_model(inputs_))\n",
    "#     prediction = torch.argmax(outputs[0]).item()\n",
    "    probability = outputs[0][CATEGORY_NUM].item()\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../rendering/shapenet_model_subsets/categories_10_models_10.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-3cc5773f0785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_3d_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimized_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcurr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcurr_pred\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcategory_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscene_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_scene_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-93a14170e6bc>\u001b[0m in \u001b[0;36mget_random_3d_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_random_3d_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mmodel_files_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../rendering/shapenet_model_subsets/categories_10_models_10.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_files_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mmodel_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mrandom_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../rendering/shapenet_model_subsets/categories_10_models_10.pkl'"
     ]
    }
   ],
   "source": [
    "model_file, category_num, category, instance = get_random_3d_model()\n",
    "optimized_params = []\n",
    "curr_pred = -1\n",
    "while curr_pred != category_num:\n",
    "    scene_params = get_random_scene_params(model_file)\n",
    "    original_scene_params = clone_scene_params(scene_params)\n",
    "    curr_pred, _ = render_and_predict(scene_params, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_neuron, selected_neuron_act = objective(scene_params, [], None)\n",
    "# selected_neuron = selected_neuron.item()\n",
    "# selected_neuron_act = selected_neuron_act\n",
    "# min_brute_force = np.min(acts['pre_final_layer'][str(selected_neuron)][category][instance])\n",
    "# max_brute_force = np.max(acts['pre_final_layer'][str(selected_neuron)][category][instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_brute_force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CMA hyperparams #####\n",
    "lam = 25\n",
    "mu = 5\n",
    "step_size = 0.15\n",
    "best, best_eval = None, 0\n",
    "all_best_evals = []\n",
    "\n",
    "#### Make initial Population\n",
    "n_children = int(lam / mu)\n",
    "population = []\n",
    "\n",
    "\n",
    "for _ in range(lam):\n",
    "    candidate = get_random_scene_params(model_file)\n",
    "    population.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_adv = False\n",
    "# for epoch in range(1):\n",
    "#     if found_adv == True:\n",
    "#         break\n",
    "    \n",
    "#     # Measure Current Population\n",
    "#     scores = []\n",
    "#     predictions = []\n",
    "#     print('Measuring current population...')\n",
    "#     for c in tqdm(population):\n",
    "#         try:\n",
    "#             score = objective(c, optimized_params, selected_neuron)\n",
    "#             scores.append(score)\n",
    "#             predictions.append(prediction)\n",
    "#         except:\n",
    "#             scores.append(1e+10)\n",
    "#             predictions.append(-1)\n",
    "#     print('Original predictions:%s'%predictions)\n",
    "\n",
    "#     # Get children for best members of population\n",
    "#     selected = np.argsort(scores)[:mu]\n",
    "#     children = list()\n",
    "#     print('Getting children...')\n",
    "#     for i in tqdm(selected):\n",
    "#         print('i is %s'%i)\n",
    "#         if scores[i] < best_eval:\n",
    "#             print('Score is %s'%scores[i])\n",
    "#             print('Prediction is %s'%predictions[i])\n",
    "#             best, best_eval = population[i], scores[i]\n",
    "#             print('Best: %.5f' % best_eval)\n",
    "#             if best_eval < min_brute_force:\n",
    "#                 print('Found new minima!')\n",
    "#                 found_adv = True\n",
    "#                 break\n",
    "#         for child_num in range(n_children):\n",
    "#             child = update_pop_member(population[i], step_size)\n",
    "#             children.append(child)\n",
    "\n",
    "# print('New minima found. Best loss=%s'%best_eval)\n",
    "# # with open('%s/%s_adv.p'%(folder, alphanumeric_key),'wb') as F:\n",
    "# #     pickle.dump(best, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Search with CMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_neuron = curr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cma\n",
    "\n",
    "cp = np.array(scene_params[1]['camera_position'])\n",
    "cat = np.array(scene_params[1]['cam_look_at'])\n",
    "fov = np.array(scene_params[1]['fov'])\n",
    "cup = np.array(scene_params[1]['cam_up'])\n",
    "start_pos = np.hstack([cp,cat,cup,fov])\n",
    "\n",
    "SCENE_PARAMS = scene_params\n",
    "CATEGORY_NUM = category_num\n",
    "\n",
    "es = cma.CMAEvolutionStrategy(start_pos, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-08d3b60bce22>:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs = softmax_layer(loaded_model(inputs_))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1     10 9.999978542327881e-01 1.0e+00 8.91e-02  9e-02  9e-02 1:26.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-b0358dd7ba06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcma_objective_adversarial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/om5/user/smadan/miniconda3/envs/diff_rendering_ml/lib/python3.8/site-packages/cma/interfaces.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, objective_fct, maxfun, iterations, min_iterations, args, verb_disp, callback, n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# deliver candidate solutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;31m# fitvals = [objective_fct(x, *args) for x in X]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mfitvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0mcevals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitvals\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# all the work is done here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om5/user/smadan/miniconda3/envs/diff_rendering_ml/lib/python3.8/site-packages/cma/optimization_tools.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, solutions, fitness_function, args, timeout)\u001b[0m\n\u001b[1;32m    269\u001b[0m                              \" passed in `__init__` or `__call__`\")\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfitness_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         warning_str = (\"`fitness_function` must be a function, not a\"\n\u001b[1;32m    273\u001b[0m                        \u001b[0;34m\" `lambda` or an instancemethod, in order to work with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om5/user/smadan/miniconda3/envs/diff_rendering_ml/lib/python3.8/site-packages/cma/optimization_tools.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    269\u001b[0m                              \" passed in `__init__` or `__call__`\")\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfitness_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         warning_str = (\"`fitness_function` must be a function, not a\"\n\u001b[1;32m    273\u001b[0m                        \u001b[0;34m\" `lambda` or an instancemethod, in order to work with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-08d3b60bce22>\u001b[0m in \u001b[0;36mcma_objective_adversarial\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mSCENE_PARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cam_look_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mscene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCENE_PARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-93a14170e6bc>\u001b[0m in \u001b[0;36mstart_up\u001b[0;34m(scene_params, optimized_params)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mscene_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlight_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaterial_settings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mscene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-93a14170e6bc>\u001b[0m in \u001b[0;36msetup_scene\u001b[0;34m(scene_params)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlight_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaterial_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mobj_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_geometry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaterial_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diffuse_material'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m#### Camera Setup ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-93a14170e6bc>\u001b[0m in \u001b[0;36mload_geometry\u001b[0;34m(model_file, geometry, mat)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mobj_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj_model_all\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mobj_model_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyredner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mobj_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj_model_all\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/load_obj.py\u001b[0m in \u001b[0;36mload_obj\u001b[0;34m(filename, obj_group, flip_tex_coords, use_common_indices, return_objects, device)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mvid0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muv_id0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_id0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vertex_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mvid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muv_id1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_id1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vertex_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mvid2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muv_id2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_id2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vertex_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/load_obj.py\u001b[0m in \u001b[0;36mget_vertex_id\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_face_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0muvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                         \u001b[0muvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_face_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0mni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "es.optimize(cma_objective_adversarial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre_final activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring current population...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/render_pytorch.py:214: UserWarning: Converting shape vertices from cpu to cuda:0, this can be inefficient.\n",
      "  warnings.warn('Converting shape vertices from {} to {}, this can be inefficient.'.format(shape.vertices.device, device))\n",
      "/net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/render_pytorch.py:216: UserWarning: Converting shape indices from cpu to cuda:0, this can be inefficient.\n",
      "  warnings.warn('Converting shape indices from {} to {}, this can be inefficient.'.format(shape.indices.device, device))\n",
      "/net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/render_pytorch.py:55: UserWarning: Converting texture from cpu to cuda:0, this can be inefficient.\n",
      "  warnings.warn('Converting texture from {} to {}, this can be inefficient.'.format(mipmap.device, device))\n",
      "100%|██████████| 10/10 [01:32<00:00,  9.20s/it]\n",
      " 20%|██        | 1/5 [00:00<00:00, 484.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores are: [1.146716833114624, 2.1038670539855957, 10000000000.0, 0.4595775008201599, 0.7915162444114685, 10000000000.0, 10000000000.0, 1.4899665117263794, 0.04131922870874405, 10000000000.0]\n",
      "Getting children...\n",
      "Score is 2.1038670539855957\n",
      "Best: 2.10387\n",
      "Score is 10000000000.0\n",
      "Best: 10000000000.00000\n",
      "Found new maxima!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "found_adv = False\n",
    "for epoch in range(2):\n",
    "    if found_adv == True:\n",
    "        break\n",
    "\n",
    "    # Measure Current Population\n",
    "    scores = []\n",
    "    print('Measuring current population...')\n",
    "    for c in tqdm_file(population):\n",
    "        try:\n",
    "            score = objective_adversarial(c, optimized_params, selected_neuron)\n",
    "            scores.append(score)\n",
    "        except:\n",
    "            scores.append(1e+10)\n",
    "    print('Scores are: %s'%scores)\n",
    "    \n",
    "    # Get children for best members of population\n",
    "    selected = np.argsort(scores)[-mu:]\n",
    "    children = list()\n",
    "    print('Getting children...')\n",
    "    for i in tqdm_file(selected):\n",
    "        if scores[i] > best_eval:\n",
    "            print('Score is %s'%scores[i])\n",
    "            best, best_eval = population[i], scores[i]\n",
    "            print('Best: %.5f' % best_eval)\n",
    "            if best_eval > max_brute_force:\n",
    "                print('Found new maxima!')\n",
    "                found_adv = True\n",
    "                break\n",
    "        for child_num in range(n_children):\n",
    "            child = update_pop_member(population[i], step_size)\n",
    "            children.append(child)\n",
    "    population = children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gif(images, titles='No Title'):    \n",
    "    ims = []\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(images)):\n",
    "        im = ax.imshow(images[i][0].cpu().permute(1,2,0).int())\n",
    "        print(titles[i])\n",
    "        ax.axis('off')\n",
    "        ims.append([im])\n",
    "        \n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=1000, blit=True,\n",
    "                                    repeat_delay=5000)\n",
    "    plt.close()\n",
    "    return HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2730636596679688, prediction: 1\n",
      "loss: 1.9682319164276123, prediction: 1\n",
      "loss: 2.0635008811950684, prediction: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\"\n",
       "href=\"https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css\">\n",
       "<script language=\"javascript\">\n",
       "  function isInternetExplorer() {\n",
       "    ua = navigator.userAgent;\n",
       "    /* MSIE used to detect old browsers and Trident used to newer ones*/\n",
       "    return ua.indexOf(\"MSIE \") > -1 || ua.indexOf(\"Trident/\") > -1;\n",
       "  }\n",
       "\n",
       "  /* Define the Animation class */\n",
       "  function Animation(frames, img_id, slider_id, interval, loop_select_id){\n",
       "    this.img_id = img_id;\n",
       "    this.slider_id = slider_id;\n",
       "    this.loop_select_id = loop_select_id;\n",
       "    this.interval = interval;\n",
       "    this.current_frame = 0;\n",
       "    this.direction = 0;\n",
       "    this.timer = null;\n",
       "    this.frames = new Array(frames.length);\n",
       "\n",
       "    for (var i=0; i<frames.length; i++)\n",
       "    {\n",
       "     this.frames[i] = new Image();\n",
       "     this.frames[i].src = frames[i];\n",
       "    }\n",
       "    var slider = document.getElementById(this.slider_id);\n",
       "    slider.max = this.frames.length - 1;\n",
       "    if (isInternetExplorer()) {\n",
       "        // switch from oninput to onchange because IE <= 11 does not conform\n",
       "        // with W3C specification. It ignores oninput and onchange behaves\n",
       "        // like oninput. In contrast, Mircosoft Edge behaves correctly.\n",
       "        slider.setAttribute('onchange', slider.getAttribute('oninput'));\n",
       "        slider.setAttribute('oninput', null);\n",
       "    }\n",
       "    this.set_frame(this.current_frame);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.get_loop_state = function(){\n",
       "    var button_group = document[this.loop_select_id].state;\n",
       "    for (var i = 0; i < button_group.length; i++) {\n",
       "        var button = button_group[i];\n",
       "        if (button.checked) {\n",
       "            return button.value;\n",
       "        }\n",
       "    }\n",
       "    return undefined;\n",
       "  }\n",
       "\n",
       "  Animation.prototype.set_frame = function(frame){\n",
       "    this.current_frame = frame;\n",
       "    document.getElementById(this.img_id).src =\n",
       "            this.frames[this.current_frame].src;\n",
       "    document.getElementById(this.slider_id).value = this.current_frame;\n",
       "  }\n",
       "\n",
       "  Animation.prototype.next_frame = function()\n",
       "  {\n",
       "    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));\n",
       "  }\n",
       "\n",
       "  Animation.prototype.previous_frame = function()\n",
       "  {\n",
       "    this.set_frame(Math.max(0, this.current_frame - 1));\n",
       "  }\n",
       "\n",
       "  Animation.prototype.first_frame = function()\n",
       "  {\n",
       "    this.set_frame(0);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.last_frame = function()\n",
       "  {\n",
       "    this.set_frame(this.frames.length - 1);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.slower = function()\n",
       "  {\n",
       "    this.interval /= 0.7;\n",
       "    if(this.direction > 0){this.play_animation();}\n",
       "    else if(this.direction < 0){this.reverse_animation();}\n",
       "  }\n",
       "\n",
       "  Animation.prototype.faster = function()\n",
       "  {\n",
       "    this.interval *= 0.7;\n",
       "    if(this.direction > 0){this.play_animation();}\n",
       "    else if(this.direction < 0){this.reverse_animation();}\n",
       "  }\n",
       "\n",
       "  Animation.prototype.anim_step_forward = function()\n",
       "  {\n",
       "    this.current_frame += 1;\n",
       "    if(this.current_frame < this.frames.length){\n",
       "      this.set_frame(this.current_frame);\n",
       "    }else{\n",
       "      var loop_state = this.get_loop_state();\n",
       "      if(loop_state == \"loop\"){\n",
       "        this.first_frame();\n",
       "      }else if(loop_state == \"reflect\"){\n",
       "        this.last_frame();\n",
       "        this.reverse_animation();\n",
       "      }else{\n",
       "        this.pause_animation();\n",
       "        this.last_frame();\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.anim_step_reverse = function()\n",
       "  {\n",
       "    this.current_frame -= 1;\n",
       "    if(this.current_frame >= 0){\n",
       "      this.set_frame(this.current_frame);\n",
       "    }else{\n",
       "      var loop_state = this.get_loop_state();\n",
       "      if(loop_state == \"loop\"){\n",
       "        this.last_frame();\n",
       "      }else if(loop_state == \"reflect\"){\n",
       "        this.first_frame();\n",
       "        this.play_animation();\n",
       "      }else{\n",
       "        this.pause_animation();\n",
       "        this.first_frame();\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.pause_animation = function()\n",
       "  {\n",
       "    this.direction = 0;\n",
       "    if (this.timer){\n",
       "      clearInterval(this.timer);\n",
       "      this.timer = null;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.play_animation = function()\n",
       "  {\n",
       "    this.pause_animation();\n",
       "    this.direction = 1;\n",
       "    var t = this;\n",
       "    if (!this.timer) this.timer = setInterval(function() {\n",
       "        t.anim_step_forward();\n",
       "    }, this.interval);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.reverse_animation = function()\n",
       "  {\n",
       "    this.pause_animation();\n",
       "    this.direction = -1;\n",
       "    var t = this;\n",
       "    if (!this.timer) this.timer = setInterval(function() {\n",
       "        t.anim_step_reverse();\n",
       "    }, this.interval);\n",
       "  }\n",
       "</script>\n",
       "\n",
       "<style>\n",
       ".animation {\n",
       "    display: inline-block;\n",
       "    text-align: center;\n",
       "}\n",
       "input[type=range].anim-slider {\n",
       "    width: 374px;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "}\n",
       ".anim-buttons {\n",
       "    margin: 8px 0px;\n",
       "}\n",
       ".anim-buttons button {\n",
       "    padding: 0;\n",
       "    width: 36px;\n",
       "}\n",
       ".anim-state label {\n",
       "    margin-right: 8px;\n",
       "}\n",
       ".anim-state input {\n",
       "    margin: 0;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<div class=\"animation\">\n",
       "  <img id=\"_anim_img80d3048268874f32b57635d888a48461\">\n",
       "  <div class=\"anim-controls\">\n",
       "    <input id=\"_anim_slider80d3048268874f32b57635d888a48461\" type=\"range\" class=\"anim-slider\"\n",
       "           name=\"points\" min=\"0\" max=\"1\" step=\"1\" value=\"0\"\n",
       "           oninput=\"anim80d3048268874f32b57635d888a48461.set_frame(parseInt(this.value));\"></input>\n",
       "    <div class=\"anim-buttons\">\n",
       "      <button title=\"Decrease speed\" onclick=\"anim80d3048268874f32b57635d888a48461.slower()\">\n",
       "          <i class=\"fa fa-minus\"></i></button>\n",
       "      <button title=\"First frame\" onclick=\"anim80d3048268874f32b57635d888a48461.first_frame()\">\n",
       "        <i class=\"fa fa-fast-backward\"></i></button>\n",
       "      <button title=\"Previous frame\" onclick=\"anim80d3048268874f32b57635d888a48461.previous_frame()\">\n",
       "          <i class=\"fa fa-step-backward\"></i></button>\n",
       "      <button title=\"Play backwards\" onclick=\"anim80d3048268874f32b57635d888a48461.reverse_animation()\">\n",
       "          <i class=\"fa fa-play fa-flip-horizontal\"></i></button>\n",
       "      <button title=\"Pause\" onclick=\"anim80d3048268874f32b57635d888a48461.pause_animation()\">\n",
       "          <i class=\"fa fa-pause\"></i></button>\n",
       "      <button title=\"Play\" onclick=\"anim80d3048268874f32b57635d888a48461.play_animation()\">\n",
       "          <i class=\"fa fa-play\"></i></button>\n",
       "      <button title=\"Next frame\" onclick=\"anim80d3048268874f32b57635d888a48461.next_frame()\">\n",
       "          <i class=\"fa fa-step-forward\"></i></button>\n",
       "      <button title=\"Last frame\" onclick=\"anim80d3048268874f32b57635d888a48461.last_frame()\">\n",
       "          <i class=\"fa fa-fast-forward\"></i></button>\n",
       "      <button title=\"Increase speed\" onclick=\"anim80d3048268874f32b57635d888a48461.faster()\">\n",
       "          <i class=\"fa fa-plus\"></i></button>\n",
       "    </div>\n",
       "    <form title=\"Repetition mode\" action=\"#n\" name=\"_anim_loop_select80d3048268874f32b57635d888a48461\"\n",
       "          class=\"anim-state\">\n",
       "      <input type=\"radio\" name=\"state\" value=\"once\" id=\"_anim_radio1_80d3048268874f32b57635d888a48461\"\n",
       "             >\n",
       "      <label for=\"_anim_radio1_80d3048268874f32b57635d888a48461\">Once</label>\n",
       "      <input type=\"radio\" name=\"state\" value=\"loop\" id=\"_anim_radio2_80d3048268874f32b57635d888a48461\"\n",
       "             checked>\n",
       "      <label for=\"_anim_radio2_80d3048268874f32b57635d888a48461\">Loop</label>\n",
       "      <input type=\"radio\" name=\"state\" value=\"reflect\" id=\"_anim_radio3_80d3048268874f32b57635d888a48461\"\n",
       "             >\n",
       "      <label for=\"_anim_radio3_80d3048268874f32b57635d888a48461\">Reflect</label>\n",
       "    </form>\n",
       "  </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "<script language=\"javascript\">\n",
       "  /* Instantiate the Animation class. */\n",
       "  /* The IDs given should match those used in the template above. */\n",
       "  (function() {\n",
       "    var img_id = \"_anim_img80d3048268874f32b57635d888a48461\";\n",
       "    var slider_id = \"_anim_slider80d3048268874f32b57635d888a48461\";\n",
       "    var loop_select_id = \"_anim_loop_select80d3048268874f32b57635d888a48461\";\n",
       "    var frames = new Array(3);\n",
       "    \n",
       "  frames[0] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\\\n",
       "bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsT\\\n",
       "AAALEwEAmpwYAAAIb0lEQVR4nO3azY9ddR3H8e+5M7ed6fRh6EyZaYcO0ziFKYyCERNBF8YoQtgQ\\\n",
       "Q0z8D4xxA0YTo2s3bo2JGkzcuNKlCZoAgVgiBBQJgq0phfJYSh8oM3ce7swc19hulMjth3m9/oLP\\\n",
       "3fze95zfadq2bQsAwnQGPQAA/hcCBkAkAQMgkoABEEnAAIgkYABEEjAAIgkYAJEEDIBIAgZAJAED\\\n",
       "IJKAARBJwACIJGAARBIwACIJGACRBAyASAIGQCQBAyCSgAEQScAAiCRgAEQSMAAiCRgAkQQMgEgC\\\n",
       "BkAkAQMgkoABEEnAAIgkYABEEjAAIgkYAJEEDIBIAgZApOFBD+Cja5pm0BPgE69t20FP4D94AgMg\\\n",
       "koABEEnAAIgkYABEEjAAIgkYAJEEDIBIAgZAJAEDIJKAARBJwACIJGAARBIwACIJGACRBAyASAIG\\\n",
       "QCQBAyCSgAEQScAAiCRgAEQSMAAiCRgAkQQMgEgCBkAkAQMgkoABEEnAAIgkYABEEjAAIgkYAJEE\\\n",
       "DIBIAgZAJAEDIJKAARBJwACIJGAARBIwACIJGACRBAyASAIGQCQBAyCSgAEQScAAiCRgAEQSMAAi\\\n",
       "CRgAkQQMgEgCBkAkAQMgkoABEEnAAIgkYABEEjAAIgkYAJEEDIBIAgZAJAEDIJKAARBJwACIJGAA\\\n",
       "RBIwACIJGACRBAyASAIGQCQBAyCSgAEQScAAiCRgAEQSMAAiCRgAkQQMgEgCBkAkAQMgkoABEEnA\\\n",
       "AIgkYABEEjAAIgkYAJEEDIBIAgZAJAEDIJKAARBJwACIJGAARBIwACIJGACRBAyASAIGQCQBAyCS\\\n",
       "gAEQScAAiCRgAEQSMAAiCRgAkQQMgEjDgx4AfLzuXry39oyP1fpGv148+7c6+97Z6n2wNuhZ8F8T\\\n",
       "MNhGpsYO1sKNx6q3tVTTozO1d3Znvbr6Ru3tTdbEjona31xXl9cuVa+3UicvvFyvvHuyLl/oDXo2\\\n",
       "XJWAwTby5Tu+UlP7pqs/tFYTO66vla3L9frm6eq367XS6dXwjgN1oJmu4bFu3b5wW/3+jd/WpfNr\\\n",
       "deq5E4OeDldwBwbbxFh3d81MHq6htlu7Ortrs7NRczvmq99br+NPPVFDHwzX1PjBmp6Yrlvnbqm5\\\n",
       "ffPV3ejU6/84PejpcFUCBtvEV++8p6b2H6pm11BVd6vadqu6zc6a3XukVlZWamF2sVbbtdrRjtSe\\\n",
       "HftqZGikPj31uerudExwbfIKEbaJt86eqcfW/lD9ZqO+vnhvdbvdGm6H6vC+I/Wd+x+q8d3j1e12\\\n",
       "62BNV1NNdZqhmt+7UNcfHq/T778z6PlwBX+tYJt49fXTdWntYvX6S7W8ulSd4aaqrRqvyXpu6Y/1\\\n",
       "o19/r374qwdraf1yNdWpoWaoZkZn69D0zKCnw1UJGGwT53rn6sD4wZq/YaFW2pVqq61mpKrWO7V7\\\n",
       "z+5aWetVb3W5Tr55opqqapqqbrdbR/bfNOjpcFUCBtvIen+tRoZGa3V5rdqtqqXeUp3ovVCH9txY\\\n",
       "+ybHqm3b+strx6vZaqq2mupv9mt1+HLN3TI16OlwBQGDbeT9CxdrfXWtLm1drLXl1brQf6/WtlZr\\\n",
       "snOobjh6oNpq688vPVnnPzhfG/3NOn32dM3turlmDnqNyLXHRxywjTz98tM1NNqpppqav+6muunQ\\\n",
       "Ys2PHquFqcX60rGv1VNHjlfbbNTGvn51Rjt17uzb1e0N19zE0Tpefx30fPgQAYNt5uj4LdXZ26mZ\\\n",
       "qbnaPzpR729erPmJm+sHDz9Ujz/zSHWaobr/tgfqu/c/WLcf+XyNjI7UwtRiHZu8o/755kv15LOP\\\n",
       "1pk3zwz6Z4CAwXbzm8cergfu+lZt3rxZE+OTdcfEF+rRZ/9Ujz/zSFVVtbVV5/rnarm/XBN7rq9D\\\n",
       "uw/XS6+8XD/++fcHvBw+zB0YbEPHZm+td157q1ZXV6rf9uvBX3y7mqappmmqO9Ste+66r0bGRurI\\\n",
       "vk/V6M5d9dPf/WTQk+EKnsBgG3p3+e0a2TNSfz/zfJ1675X6xt3frKHq1Gq7VjPjM7W+sl6nzp6s\\\n",
       "o7ML9ctHflYvnHp20JPhCk3btu2gR/DRNE0z6Al8gizOfrbu/MwXa+fYSM3vP1r/On+iXnzj+Xri\\\n",
       "qccGPW2gHJXXHgH7BBAw+P9zVF573IEBEEnAAIgkYABEEjAAIgkYAJEEDIBIAgZAJAEDIJKAARBJ\\\n",
       "wACIJGAARBIwACIJGACRBAyASAIGQCQBAyCSgAEQScAAiCRgAEQSMAAiCRgAkQQMgEgCBkAkAQMg\\\n",
       "koABEEnAAIgkYABEEjAAIgkYAJEEDIBIAgZAJAEDIJKAARBJwACIJGAARBIwACIJGACRBAyASAIG\\\n",
       "QCQBAyCSgAEQScAAiCRgAEQSMAAiCRgAkQQMgEgCBkAkAQMgkoABEEnAAIgkYABEEjAAIgkYAJEE\\\n",
       "DIBIAgZAJAEDIJKAARBJwACIJGAARBIwACIJGACRBAyASAIGQCQBAyCSgAEQScAAiCRgAEQSMAAi\\\n",
       "CRgAkQQMgEgCBkAkAQMgkoABEEnAAIgkYABEEjAAIgkYAJEEDIBIAgZAJAEDIJKAARBJwACIJGAA\\\n",
       "RBIwACIJGACRBAyASAIGQCQBAyCSgAEQScAAiCRgAEQSMAAiCRgAkQQMgEgCBkAkAQMgkoABEEnA\\\n",
       "AIgkYABEEjAAIgkYAJEEDIBIAgZAJAEDINLwoAfw0bVtO+gJAB87T2AARBIwACIJGACRBAyASAIG\\\n",
       "QCQBAyCSgAEQScAAiCRgAEQSMAAiCRgAkQQMgEgCBkAkAQMgkoABEEnAAIgkYABEEjAAIgkYAJEE\\\n",
       "DIBIAgZAJAEDIJKAARBJwACIJGAARBIwACIJGACRBAyASAIGQCQBAyCSgAEQScAAiCRgAEQSMAAi\\\n",
       "CRgAkf4NhjVE/vtGASYAAAAASUVORK5CYII=\\\n",
       "\"\n",
       "  frames[1] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\\\n",
       "bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsT\\\n",
       "AAALEwEAmpwYAAAIEUlEQVR4nO3bzYtddx3H8c+ZzCSTiUknD01qtU0f0FpQMcXWttJWEYIouBCk\\\n",
       "GxcquHElgn+If4BIKYgi4qIIQqE+FqQgllqj1rbGprFJzLSTpzaZmcxxcQuCC4UWc+bTvl67yz0X\\\n",
       "vr/FPW9+52EYx3EMAJSZm3oAAHgrBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoC\\\n",
       "BkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACV\\\n",
       "BAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgErzUw/A2zcMw9QjwDveOI5Tj8B/sAMD\\\n",
       "oJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoC\\\n",
       "BkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACV\\\n",
       "BAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAA\\\n",
       "KgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRg\\\n",
       "AFQSMAAqCRgAlQQMtoBhbsj7bjvwP487sC+59eZrMBAUEDCY2pAcuuWGLC7v/a+HHTo4n/s/sSPX\\\n",
       "77tGc8EWNz/1APBut20uOfrx09kczuS+O4Zcen13Nja3ZX1jLufeWMz6RjI3jLnj8Bu5af9qnvzt\\\n",
       "1BPD1iBgMKFhSB64Ozn6yc3sXkrm55OM55NhyPp68spK8scXk317kjtvTz58x5hLF5PvfHfqyWF6\\\n",
       "AgYTmptLPvup5OljyX1HksXtybZtyZAxiwvJ0mKyayF54R+zmO1+JfnyF5NzFxZy/B9DVlbW8oc/\\\n",
       "Tb0KmIZ7YDChzzyQ7F5KdmxPXjqRnLv05hfDbHc2vy258Ybk3o8ld96aLC0kh/Yn3/r6eg4sr+XZ\\\n",
       "P085PUzLDgwmMgzJPXcljz6WfOnzycJ88tTTycFDydnTyfaFZPv2ZPd7kuXlZEhy+Kbk5OnkpZPJ\\\n",
       "pUvJOE68CJjQMI7+Au2GYZh6BN6CI3cfysG927J6fj1P//6f+fY3ZsF65PvJCydmx+zfO9ulnX01\\\n",
       "efF48oWjyeH3J8f+mvzoseT8xUmX8K7iVLn1uIQIExl3X5+/nNzM6ricPQeuyyM/XsyF1X/HK0kW\\\n",
       "d8wuLz7xm+T4y8njv05eOZMcPyFe4BIiTOBzn04euu/ZXLySrG+cypW15PKV5PSlxTx49GD27dnM\\\n",
       "jvmr+cDN61necTaPvvm7YS559vn5/OKpIcn6lEuAyQkYTOD+u5Lzr87uaa2Ps13WwlyyOV7O6oWX\\\n",
       "8rtjyXv3Jw8eSW6/OTn2XPKDx+ayf99csrGRq2tTrwCmJ2BwjX30g8nVMdm5lJy7kCzuTDY2kmFh\\\n",
       "9hj93uuSez6SXN5IXj6V7NqZfPNryc6lzZxZ2czjv5x6BbA1uAcG19gzzyXP/212f2thIVl5LXn9\\\n",
       "jdmDGmdfTdauJLuWkoPLyZ6ds6jt25t89eHk6mayem7qFcDWYAcGE3j0J8nCjuTGg8mu5eSnP082\\\n",
       "xiEfun3M5jifjbWNXL6c3Htk9rj96TPJU88kv3py6slh6xAwmMj3fph85eFkWEnOXxxy6y17Mlw9\\\n",
       "l7W1jbx8OhmvJsdPJqfOzC45vvj35OxrU08NW4f3wN4BvAfWbxiS2w4ny9fNPo+byZhk2ExWVpOj\\\n",
       "DyU/eyI5cWrKKd/dnCq3HgF7BxAw+P9zqtx6PMQBQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQM\\\n",
       "gEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJ\\\n",
       "GACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABU\\\n",
       "EjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAA\\\n",
       "qCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKA\\\n",
       "AVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAl\\\n",
       "AQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyA\\\n",
       "SgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkY\\\n",
       "AJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQS\\\n",
       "MAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZApfmpB+DtG8dx6hEA\\\n",
       "rjk7MAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJ\\\n",
       "wACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOg\\\n",
       "koABUEnAAKgkYABUEjAAKgkYAJUEDIBK/wLKEhe4cZpL5gAAAABJRU5ErkJggg==\\\n",
       "\"\n",
       "  frames[2] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\\\n",
       "bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsT\\\n",
       "AAALEwEAmpwYAAAIUUlEQVR4nO3cy26dVx3G4Xc7ju04sROnObhJW5pUKbQhqKiCVoUCogyRGMEE\\\n",
       "MYQ74QJ6B6gTRMUUZpQBgiKgCiWESglpmqTHJG0cp3FOtjeDPavEIG7L9mueR/LIk/+SvNdPa/nb\\\n",
       "32A4HA4DAGUmxj0AAGyEgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABU\\\n",
       "EjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAA\\\n",
       "qCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUGly3APw6Q0Gg3GPAFvecDgc9wh8ghMYAJUEDIBK\\\n",
       "AgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgA\\\n",
       "lQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIw\\\n",
       "ACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgk\\\n",
       "YABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQ\\\n",
       "ScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUmxz0A8PmYmEh2\\\n",
       "zU9n4fCBbL/3Xs6fW83a2rings+OgMEWs21i9POT7yZPHLmT356+lNfPTmeYtSTDcY8Hn5nBcDj0\\\n",
       "F11uMBiMewQ2gcEgmdyWvPDV5HtPJatryZH9yfRM8ptTySun5nLh/MdZXfWR3whb5eYjYFuAgDGY\\\n",
       "GOTBg4M88+R6Fnclc1PJzh3Jk48kC7Ojc9c/3k5+9Zddef3Ux7l3b5D1dR/9+2Gr3HwEbAsQsP9f\\\n",
       "U5PJ9FSysLgn+w7uzPrKtezatpJvPJ4c3JM8uDvZP59s25YMkry/nPziz8m/Lszl3Ys3s7Y6zDDJ\\\n",
       "7NQwK3fGvJhNzla5+fgfGJSa3p5860Ty3PFkYnIpf3/vXt68uyOv/3s1j+27m9nZZP2jZHYqmZsd\\\n",
       "XTEemE+eP5oszt/Iyfk9uXb1dlZXJ/LcEyv59SvjXhHcHwGDUiceSZ46kqzeTR6YSb599Gb2zd7M\\\n",
       "+p2ZrA4ncn15PWeuJWcvJc9/OVlcSLZPJod3J1c/Tn749FIePpi8czl58eVxrwbun4BBqcvLyfJK\\\n",
       "cvRwMjOTTAySpx9K9u+8naVbyct/nMx7V9aztrqe0xeTHzyTHH80ObQnOXdldMX4428mP3sx+eD6\\\n",
       "mBcDG+CLzFDq4pXk7avJ2npydzXJMJmcSI7tTw7NJ48dm8tDj8xkZnYyr56ZyEu/T06+ldy4nXz9\\\n",
       "WLI2TG7dSRb3jXslsDFOYFBs+Xby0Upy/XYyP5PMziRz08mj+5LHr1/LgV3JpQdmM7y7nmP7b+e1\\\n",
       "N5M/nUp++v3k6SOj+H1hX3JkMTn//rhXA/dHwKDY1evJhQ+S6yvJ3p3JsUPJ5J7RAxu7tiUHHkhe\\\n",
       "OL6SwwvJz19K/npmFK3nTiQ7ppKl3cnCXPLscQGjj4BBsTcuJh8uJ48+mOydS67eGH3na+9ccvRA\\\n",
       "MjGZfOXhZHoyOfvuKF5Jcms1+cMbyWvnklNvJb/83ThXARsjYFDu8lIyM5XM7Ux2bE/OX0ne/TB5\\\n",
       "9kujK8U7d5NDC6OHPB7aPzpxXb+ZXLqcvPrP5IOlca8ANkbAYAu4eDlZvpncvZVMTCUzk8mV5eTE\\\n",
       "3mT3jmT3zuRrX0xmdyQ3VpJXTianz497avh0BAy2iKWbyclzyfyu0ePy71xNZqZHv7u9lhxcSE5f\\\n",
       "SP52JllfH+uo8JnwKqktwKuk+KTFvaOHNH70nWQ4TN64NLou/HDZ++g3yla5+QjYFiBg/DfPPDG6\\\n",
       "Wjz7zujt9GycrXLzEbAtQMDg82er3Hy8iQOASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQM\\\n",
       "gEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJ\\\n",
       "GACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABU\\\n",
       "EjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAA\\\n",
       "qCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKA\\\n",
       "AVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAl\\\n",
       "AQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyA\\\n",
       "SgIGQCUBA6CSgAFQScAAqCRgAFQSMAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkY\\\n",
       "AJUEDIBKAgZAJQEDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqDQ57gH4\\\n",
       "9IbD4bhHAPifcwIDoJKAAVBJwACoJGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqCRgAFQS\\\n",
       "MAAqCRgAlQQMgEoCBkAlAQOgkoABUEnAAKgkYABUEjAAKgkYAJUEDIBKAgZAJQEDoJKAAVBJwACo\\\n",
       "JGAAVBIwACoJGACVBAyASgIGQCUBA6CSgAFQScAAqPQfz4ZAXuJju/0AAAAASUVORK5CYII=\\\n",
       "\"\n",
       "\n",
       "\n",
       "    /* set a timeout to make sure all the above elements are created before\n",
       "       the object is initialized. */\n",
       "    setTimeout(function() {\n",
       "        anim80d3048268874f32b57635d888a48461 = new Animation(frames, img_id, slider_id, 1000.0,\n",
       "                                 loop_select_id);\n",
       "    }, 0);\n",
       "  })()\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_gif(all_rendered_inputs, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it go up if the learning rate is low, shouldn't it work out over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

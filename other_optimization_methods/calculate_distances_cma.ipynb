{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "needed-sentence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/__init__.py\n",
      "/net/storage001.ib.cluster/om2/user/smadan/redner/pyredner/__init__.py\n",
      "/net/storage001.ib.cluster/om2/user/smadan/training_scaffold_own/res/loader/multi_attribute_loader.py\n",
      "/net/storage001.ib.cluster/om2/user/smadan/training_scaffold_own/res/loader\n",
      "/net/storage001.ib.cluster/om2/user/smadan/training_scaffold_own/res/loader/loader.py\n",
      "/net/storage001.ib.cluster/om2/user/smadan/training_scaffold_own/res/loader\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../inverse_rendering/')\n",
    "machine_path = os.getcwd()\n",
    "user_root_dir = '/'.join(machine_path.split('/')[:-2])\n",
    "sys.path.insert(0,'%s/redner/'%user_root_dir)\n",
    "import redner\n",
    "import pyredner\n",
    "print(pyredner.__file__)\n",
    "pyredner.render_pytorch.print_timing = False\n",
    "from general_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "incredible-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folders = [\"cma_adversarial_cam/neurips_results/%s\"%i for i in os.listdir('cma_adversarial_cam/neurips_results/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "white-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_scene_params(scene_params):\n",
    "    original_scene_params = [0,0,0,0]\n",
    "    original_scene_params[0] = scene_params[0]\n",
    "\n",
    "    original_scene_params[1] = {}\n",
    "    for key in scene_params[1].keys():\n",
    "        original_scene_params[1][key] = scene_params[1][key].clone()\n",
    "\n",
    "    original_scene_params[2] = {}\n",
    "    for key in scene_params[2].keys():\n",
    "        original_scene_params[2][key] = [i.clone() for i in scene_params[2][key]]\n",
    "\n",
    "    original_scene_params[3] = scene_params[3]\n",
    "    return original_scene_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "stable-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_params(initial_params, x):\n",
    "    clone = clone_scene_params(initial_params)\n",
    "    clone[1]['camera_position'] = torch.tensor(x[:3]).float()\n",
    "    clone[1]['cam_look_at'] = torch.tensor(x[3:6]).float()\n",
    "    clone[1]['cam_up'] = torch.tensor(x[6:9]/10).float()\n",
    "    clone[1]['fov'] = torch.tensor(x[9]*10).float()\n",
    "    return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "south-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_params_light(initial_params, x):\n",
    "    clone = clone_scene_params(initial_params)\n",
    "    clone[1]['camera_position'] = torch.tensor(x[:3]).float()\n",
    "    clone[1]['cam_look_at'] = torch.tensor(x[3:6]).float()\n",
    "    clone[1]['cam_up'] = torch.tensor(x[6:9]/10).float()\n",
    "    clone[1]['fov'] = torch.tensor(x[9]*10).float()\n",
    "    return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eight-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(initial_params,final_params):\n",
    "    pos_dist = torch.linalg.norm(initial_params[1]['camera_position'] - final_params[1]['camera_position'],1)/(3*4)*100\n",
    "    look_dist = torch.linalg.norm(initial_params[1]['cam_look_at'] - final_params[1]['cam_look_at'],1)/(3*4*0.3)*100\n",
    "    return pos_dist, look_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "southeast-conversion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "        True])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(exp_info['es'].predictions) != exp_info['es'].correct_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "offensive-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result_folder in results_folders:\n",
    "#     exp_files = ['%s/%s'%(result_folder,i) for i in os.listdir(result_folder) if i.endswith('.p')]\n",
    "#     err_exp = 0\n",
    "#     total_exp = 0\n",
    "#     for exp_file in exp_files:\n",
    "#         total_exp += 1\n",
    "#         with open(exp_file,'rb') as F:\n",
    "#             exp_info = pickle.load(F)\n",
    "#             errors = np.array(exp_info['es'].predictions) != exp_info['es'].correct_prediction\n",
    "#             if len(errors)>0:\n",
    "#                 err_exp += 1\n",
    "#     print(err_exp/total_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "concrete-equation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cma_adversarial_cam/neurips_results/resnet18_v7_normalized_final 1.7481127 +- 1.7060299 5.833493 +- 5.902203\n",
      "cma_adversarial_cam/neurips_results/resnet18_pretrained_v7_normalized_final 1.7129765 +- 1.7403842 5.6361227 +- 4.6827474\n",
      "cma_adversarial_cam/neurips_results/resnet18_antialiased_v7_normalized_final 3.293728 +- 3.8250768 10.467619 +- 10.664044\n",
      "cma_adversarial_cam/neurips_results/resnet18_v7_truly_shift_invariant_normalized_final 2.2782829 +- 2.3509247 6.787909 +- 6.428416\n",
      "cma_adversarial_cam/neurips_results/train_v7_transformer_2_final 1.3181101 +- 0.8682036 4.29823 +- 2.3946288\n",
      "cma_adversarial_cam/neurips_results/train_v7_transformer_2_deit 1.2455395 +- 0.78311616 4.4083495 +- 3.2457292\n",
      "cma_adversarial_cam/neurips_results/train_v7_transformer_2_deit_distilled 1.2083895 +- 0.68837905 4.2269783 +- 2.8569345\n",
      "cma_adversarial_cam/neurips_results/resnet18_v7_40_final 1.8336893 +- 1.3259414 6.5231795 +- 5.684479\n",
      "cma_adversarial_cam/neurips_results/resnet18_pretrained_v7_40_normalized_final 1.7861087 +- 1.4620334 5.3622446 +- 3.6965773\n",
      "cma_adversarial_cam/neurips_results/resnet18_antialiased_v7_40_normalized_final 2.3226557 +- 2.0947073 7.027734 +- 5.1037908\n",
      "cma_adversarial_cam/neurips_results/resnet18_v7_40_truly_shift_invariant_normalized 2.2218945 +- 2.1623037 6.7181554 +- 5.413413\n",
      "cma_adversarial_cam/neurips_results/train_v7_transformer_40_2 1.341537 +- 1.1608803 4.625784 +- 3.4909382\n",
      "cma_adversarial_cam/neurips_results/train_v7_transformer_40_2_deit 1.2739452 +- 0.8145107 4.537012 +- 2.7468753\n",
      "cma_adversarial_cam/neurips_results/train_v7_transformer_40_2_deit_distilled 1.2161833 +- 0.8718089 4.4931173 +- 2.2707052\n"
     ]
    }
   ],
   "source": [
    "meta_all_cd = []\n",
    "meta_all_ld = []\n",
    "for result_folder in results_folders:\n",
    "    exp_files = ['%s/%s'%(result_folder,i) for i in os.listdir(result_folder) if i.endswith('.p')]\n",
    "    all_cd = []\n",
    "    all_ld = []\n",
    "    for exp_file in exp_files:\n",
    "        with open(exp_file,'rb') as F:\n",
    "            exp_info = pickle.load(F)\n",
    "            initial = exp_info['scene_params']\n",
    "            adv_positions = np.where(np.array(exp_info['es'].predictions) != exp_info['es'].correct_prediction)[0]\n",
    "            if len(adv_positions)>0:\n",
    "                adv_position = adv_positions[0]\n",
    "                adv_setting = exp_info['es'].prediction_settings[adv_position]\n",
    "                final = get_final_params(initial, adv_setting)\n",
    "                cd, ld = calculate_distances(initial, final)\n",
    "                all_cd.append(cd)\n",
    "                all_ld.append(ld)\n",
    "                \n",
    "    print(result_folder, \"%s +- %s\"%(np.mean(all_cd), np.std(all_cd)), \"%s +- %s\"%(np.mean(all_ld), np.std(all_ld)))\n",
    "    meta_all_cd.append(np.mean(all_cd))\n",
    "    meta_all_ld.append(np.mean(all_ld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "banner-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(all_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "impressed-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.std(all_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "exceptional-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "comic-confusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "decent-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances_simple(param_1,param_2, max_range):\n",
    "    return torch.linalg.norm(torch.FloatTensor(param_1) - torch.FloatTensor(param_2),1)/(3*max_range)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "regional-trance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2500)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_distances_simple([-1.15,0.97,-0.79],[-0.95,1.3,-0.89],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "applied-checklist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.5000)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_distances_simple([-1.15,0.97,-0.79],[-0.95,1.3,-0.89],1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "provincial-england",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1667)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_distances_simple([0.38,0.9,1.29],[0.37,1,1.32],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "catholic-spider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.8889)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_distances_simple([0.22,0.43,0.3],[0.16,0.23,0.36],1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "understood-thomson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9167)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_distances_simple([1.44, -0.1,0.78],[1.48,-0.09,0.84],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "pursuant-sheet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3333)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_distances_simple([0.15,0.04,0.1],[0.13,-0.03,0.13],1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "destroyed-fortune",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2500)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_distances_simple([-1.32,-0.65,-0.68],[-1.25,-0.72,-0.69],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "agreed-environment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7778)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_distances_simple([0.26,0.37,0.04],[0.3,0.33,0.06],1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-setting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_rendering_ml",
   "language": "python",
   "name": "diff_rendering_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
